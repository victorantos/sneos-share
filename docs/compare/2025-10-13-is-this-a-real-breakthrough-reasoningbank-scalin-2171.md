---
title: "is this a real breakthrough: REASONINGBANK: Scalin..."
description: "AI model comparison: is this a real breakthrough: REASONINGBANK: Scaling Agent Self-Evolving with Reasoning Memory Siru O..."
date: 2025-10-13
category: comparison
layout: Clean
tags:
  - Gemini
  - Grok
  - ChatGPT
  - AI Comparison
author: SNEOS Community
head:
  - - meta
    - name: description
      content: "AI model comparison: is this a real breakthrough: REASONINGBANK: Scaling Agent Self-Evolving with Reasoning Memory Siru Ouyang!* , Jun Yan~ '..."
---

<ChatComparison
  title="is this a real breakthrough: REASONINGBANK: Scalin..."
  prompt="is this a real breakthrough: REASONINGBANK: Scaling Agent&#10;Self-Evolving with Reasoning Memory&#10;Siru Ouyang!*&#10;, Jun Yan~&#10;', I-Hung Hsu,&#10;, Yanfei Chen'&#10;, Ke Jiang?, Zifeng Wang?, Rujun Han?, Long T. Le?, Samira Daruki?, Xiangru Tang3, Vishy Tirumalashetty?, George Lee?, Mahsan Rofoueit, Hangfei Lin, Jiawei&#10;Han'&#10;, Chen-Yu Lee? and Tomas Pfister?&#10;1 University of Illinois Urbana-Champaign, Google Cloud Al Research,&#10;, 3 Yale University, *Google Cloud Al With the growing adoption of large language model agents in persistent real-world roles, they naturally encounter continuous streams of tasks. A key limitation, however, is their failure to learn from the accumulated interaction history, forcing them to discard valuable insights and repeat past errors. We propose REASONIngBank, a novel memory framework that distills generalizable reasoning strategies from an agent's self-judged successful and failed experiences. At test time, an agent retrieves relevant memories from REASONINgBANK to inform its interaction and then integrates new learnings back, enabling it to become more capable over time. Building on this powerful experience learner, we further introduce memory-aware test-time scaling (MATTS), which accelerates and diversifies this learning process by scaling up the agent's interaction experience. By allocating more compute to each task, the agent generates abundant, diverse experiences that provide rich contrastive signals for synthesizing higher-quality memory. The better memory in turn guides more effective scaling, establishing a pow- erful synergy between memory and test-time scaling. Across web browsing and software engineering benchmarks, REASONINGBANk consistently outperforms existing memory mechanisms that store raw trajectories or only successful task routines, improving both effectiveness and efficiency; MATTS further amplifies these gains. These findings establish memory-driven experience scaling as a new scaling&#10;dimension, enabling agents to self-evolve with emergent behaviors naturally arise."
  :responses='{
    "Gemini": "Whether \u0022REASONINGBANK: Scaling Agent Self-Evolving with Reasoning Memory\u0022 is a real breakthrough is subjective and depends on how one defines \u0022breakthrough.\u0022 However, based on the abstract provided, here\u0027s an analysis of why it could be considered a significant advancement:\n\n*   **Addresses a key limitation of LLM agents:** The paper identifies that current LLM agents struggle to learn from accumulated interaction history, leading to repeated errors and discarded insights. REASONINGBANK directly tackles this issue.\n*   **Novel memory framework:** REASONINGBANK introduces a new approach to memory by distilling generalizable reasoning strategies from both successful and failed experiences. This is different from existing memory mechanisms that store raw trajectories or only successful task routines.\n*   **Improved performance:** The abstract claims that REASONINGBANK consistently outperforms existing memory mechanisms on web browsing and software engineering benchmarks, improving both effectiveness and efficiency.\n*   **Memory-aware test-time scaling (MATTS):** The introduction of MATTS further amplifies the gains of REASONINGBANK by scaling up the agent\u0027s interaction experience, leading to higher-quality memory and more effective scaling.\n*   **Self-evolution:** The findings suggest that memory-driven experience scaling enables agents to self-evolve with emergent behaviors.\n\nTo further assess if this is a \u0022real breakthrough,\u0022 it would be helpful to consider the following:\n\n*   **Peer review and validation:** Has this paper been peer-reviewed and published in a reputable scientific venue?\n*   **Reproducibility:** Can the results be reproduced by other researchers?\n*   **Impact on the field:** Has this work had a significant impact on the field of AI and LLM agents? Are other researchers building upon this work?\n\nIn conclusion, REASONINGBANK seems to be a significant advancement in addressing the limitations of LLM agents in learning from experience. The proposed memory framework and MATTS contribute to improved performance and self-evolution capabilities. However, whether it\u0027s a \u0022real breakthrough\u0022 requires further evaluation based on peer review, reproducibility, and impact on the field.\n\n",
    "Grok": "Yes, ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory appears to be a legitimate and significant advancement in AI research, particularly for large language model (LLM) agents. I\u0027ll break this down based on available information, including the paper you referenced, to explain why it\u0027s considered a breakthrough.\n\n### Is It Real?\n- **Publication Details**: This is a real academic paper titled \u0022[2509.25140] ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory,\u0022 authored by Siru Ouyang (lead, affiliated with University of Illinois Urbana-Champaign and Google Cloud AI), along with collaborators like Jun Yan, I-Hung Hsu, Yanfei Chen, and others from UIUC, Google Cloud AI Research, and Yale University. It was published on arXiv on September 29, 2025, and is openly accessible there. The abstract you provided matches the paper\u0027s content exactly, describing a novel memory framework for LLM agents to learn from both successes and failures in ongoing tasks.\n- **Credibility**: The authors are affiliated with reputable institutions. Google Cloud AI Research\u0027s involvement adds weight, as they\u0027ve been pushing boundaries in agentic AI. No signs of it being fabricated\u2014it\u0027s indexed on platforms like ResearchGate and has been peer-discussed in AI communities.\n\n### Why Is It a Breakthrough?\nReasoningBank addresses a core limitation in current LLM agents: they often treat each interaction in isolation, forgetting past lessons and repeating mistakes. This paper introduces innovative mechanisms to make agents \u0022self-evolving,\u0022 which is a big deal for real-world applications like web browsing, software engineering, or persistent AI assistants. Key highlights from the paper:\n\n- **Core Innovation (ReasoningBank)**: It\u0027s a memory system that distills \u0022generalizable reasoning strategies\u0022 from an agent\u0027s experiences\u2014both successful and failed. Instead of just storing raw data (like full interaction logs), it extracts high-level, transferable insights (e.g., \u0022Avoid clicking unverified links in web navigation\u0022 from a past error). At runtime:\n  - The agent retrieves relevant memories to guide decisions.\n  - It updates the memory bank with new learnings, allowing continuous improvement without retraining the underlying model.\n  This enables emergent behaviors, like adapting to new tasks based on historical patterns, which outperforms baselines that only save successful routines or raw trajectories.\n\n- **Extension (Memory-Aware Test-Time Scaling or MaTTS)**: Building on ReasoningBank, this scales up compute during inference (test time) to generate more diverse experiences per task. These richer experiences feed back into better memory synthesis, creating a virtuous cycle. It\u0027s efficient\u2014improving performance without massive pre-training costs\u2014and shows gains on benchmarks like WebArena (for web tasks) and SWE-Bench (for software engineering).\n\n- **Results and Impact**: Experiments demonstrate consistent improvements in success rates (e.g., higher accumulative performance on multi-task sequences) and efficiency (fewer tokens needed). It establishes \u0022memory-driven experience scaling\u0022 as a new paradigm, potentially reducing the need for ever-larger models by making agents smarter through interaction history.\n\n### Reception and Coverage\n- **In the AI Community**: It\u0027s been hailed as a step toward more autonomous, adaptive AI agents. For instance, a MarkTechPost article from October 1, 2025, describes it as a \u0022strategy-level AI agent memory framework that makes LLM agents self-evolve at test time,\u0022 emphasizing its role in handling real-world unpredictability. VentureBeat (October 9, 2025) called it a framework that \u0022turns every interaction into a learning opportunity,\u0022 highlighting cost savings for deploying agents in production.\n- **Broader Implications**: This could accelerate AI in areas like robotics, customer service bots, or dev tools, where agents need to learn on the fly. It\u0027s part of a 2025 trend in \u0022agentic AI\u0022 (e.g., similar to work on reflection or self-critique in models like o1), but ReasoningBank\u0027s focus on failure learning and synergy with scaling sets it apart. Early reviews on platforms like The Moonlight (October 1, 2025) praise it for bridging gaps in existing memory systems like trajectory storage.\n\nIf this is hype or not fully revolutionary depends on perspective\u2014AI memory isn\u0027t entirely new (e.g., inspired by vector databases or reflection mechanisms)\u2014but the paper\u0027s empirical results and novelty in distilling strategies from failures make it a meaningful contribution. It\u0027s still early (just two weeks post-publication), so real-world adoption will tell, but it\u0027s generating buzz in conferences and labs.\n\nIf you have the full paper or more context, I can dive deeper into specifics like the architecture or comparisons!",
    "ChatGPT": "The paper you describe, \u0022REASONINGBANK: Scaling Agent Self-Evolving with Reasoning Memory,\u0022 proposes a novel approach to improving large language model (LLM) agents by enabling them to learn continuously from accumulated interaction history through a reasoning memory framework. This addresses a known limitation in current LLM agents that often discard past experiences and therefore fail to improve from them.\n\nKey innovations include:\n\n1. **ReasoningBank Memory Framework:** Instead of just storing raw interactions or only successful task routines, the system distills generalized reasoning strategies from both successes and failures. This allows the agent to retrieve relevant past knowledge to guide current decisions and to integrate new learnings back into memory, effectively enabling continual self-improvement.\n\n2. **Memory-Aware Test-Time Scaling (MATTS):** This technique allocates more computation per task to generate more diverse and abundant experiences, which improves the quality of memory and makes scaling more effective. It creates a synergy between memory and scaling, which is novel.\n\n3. **Empirical Validation:** The approach reportedly outperforms existing memory mechanisms on benchmarks in web browsing and software engineering, both in terms of effectiveness and efficiency.\n\n### Is this a real breakthrough?\n\n- **Conceptual Novelty:** The idea of a memory system that not only stores past experiences but distills reasoning strategies from both successful and failed attempts is quite compelling. Previous efforts often rely on replaying raw data or only positive outcomes, which limits generalizability and learning potential. Integrating self-judgement and failure learning is a meaningful advancement.\n\n- **Practical Significance:** If the empirical results hold under broad conditions, this approach could substantially improve the usability and autonomy of LLM agents in persistent, real-world tasks by enabling continual learning and adaptation without retraining from scratch.\n\n- **Scaling Aspect:** The introduction of a new \u0022scaling dimension\u0022 focused on experience quality and memory-driven learning at test time is an interesting contribution that complements existing model size and data scaling.\n\n- **Emergent Behaviors:** The claim that emergent behaviors naturally arise from the synergy between memory and scaling is notable, as emergence is a key aspect of advanced AI systems.\n\n### Caveats\n\n- Replication and independent validation will be important to confirm the robustness of these results.\n\n- Real-world deployment challenges such as memory storage costs, forgetting mechanisms, and ensuring memory quality need further exploration.\n\n- Comparisons to other continual learning and memory-augmented agent frameworks will help position this work clearly.\n\n### Summary\n\nThis work represents a promising and potentially impactful advance in the development of self-evolving agents powered by large language models. It addresses a critical weakness in current systems and proposes a creative solution with supporting empirical evidence. Whether it qualifies as a definitive \u0022breakthrough\u0022 will depend on further validation and adoption, but it is certainly a significant and interesting contribution to AI research in agent memory and scaling."
  }'
  published-date="10:04"
/>

## Analysis

This comparison demonstrates the different approaches each AI model takes when responding to the same prompt. Here are the key differences observed:

### Response Characteristics

**Gemini**: Provides a detailed response with 15 sentences. 

**Grok**: Provides a detailed response with 40 sentences. 

**ChatGPT**: Provides a detailed response with 23 sentences. 

## Key Takeaways

- Each model brings unique strengths to this type of query
- Response styles vary significantly between models
- Consider your specific use case when choosing between these models

## Try This Comparison Yourself

Want to test these models with your own prompts? Visit [SNEOS.com](https://sneos.com) to compare AI responses side-by-side in real-time.

---

*This comparison was generated using the [SNEOS AI Comparison Tool](https://sneos.com)*
*Published: October 13, 2025 | Models: Gemini, Grok, ChatGPT*
