---
title: "reducing ediscovery data volumes"
description: "AI model comparison: reducing ediscovery data volumes..."
date: 2025-10-01
category: comparison
layout: Clean
tags:
  - ChatGPT
  - DeepSeek
  - Gemini
  - AI Comparison
author: SNEOS Community
head:
  - - meta
    - name: description
      content: "AI model comparison: reducing ediscovery data volumes..."
---

<ChatComparison
  title="reducing ediscovery data volumes"
  prompt="reducing ediscovery data volumes"
  :responses='{
    "ChatGPT": "Reducing eDiscovery data volumes is critical for minimizing legal costs, improving review efficiency, and ensuring faster case resolution. Here are some effective strategies to reduce eDiscovery data volumes:\n\n### 1. **Implement Early Case Assessment (ECA)**\n- Use ECA tools to quickly analyze and filter data before full processing.\n- Identify relevant data early and exclude non-responsive documents.\n\n### 2. **Apply Targeted Data Collection**\n- Avoid collecting entire data repositories; instead, collect data from specific custodians, time frames, or topics.\n- Use custodian interviews and questionnaires to focus data collection.\n\n### 3. **Use Data Filtering and Deduplication**\n- Apply date, keyword, and file type filters at the collection stage.\n- Deduplicate data to remove identical copies of documents.\n\n### 4. **Leverage Technology-Assisted Review (TAR)**\n- Use machine learning and AI to prioritize documents, reducing the volume requiring human review.\n- TAR can significantly cut down the number of documents reviewed manually.\n\n### 5. **Exclude Irrelevant Data Types**\n- Exclude system files, benign file types (e.g., executables), and known irrelevant data (e.g., personal files).\n- Filter out large media files unless relevant.\n\n### 6. **Implement Effective Data Retention and Deletion Policies**\n- Enforce policies to delete unnecessary data before litigation arises.\n- Regularly cleanse email archives and shared drives.\n\n### 7. **Use Selective Preservation**\n- Preserve only data relevant to the litigation or investigation.\n- Avoid over-preserving broad datasets.\n\n### 8. **Collaborate with IT and Legal Teams**\n- Close coordination ensures proper data identification, preservation, and collection procedures.\n- Helps avoid over-collection.\n\n### 9. **Continuous Data Hygiene**\n- Maintain regular data management practices to prevent data sprawl.\n- Archive or delete outdated data as part of routine operations.\n\n---\n\n### Summary\nReducing eDiscovery data volumes involves a combination of targeted collection, early assessment, use of advanced analytics, and good data governance. Employing these best practices helps streamline the eDiscovery process and control costs.\n\nIf you want, I can also provide specific tools or workflow examples. Would that be helpful?",
    "DeepSeek": "Of course. Reducing eDiscovery data volumes is one of the most critical and cost-effective strategies in modern legal practice. High data volumes directly translate to exorbitant costs for processing, review, and hosting.\n\nHere is a comprehensive guide to reducing eDiscovery data volumes, broken down into a strategic framework.\n\n### The Core Principle: \u0022Right-Sizing\u0022 the Collection\n\nThe goal is not to collect *everything*, but to collect *the right things*. Data reduction is most effective when applied early in the Electronic Discovery Reference Model (EDRM) lifecycle, primarily at the **Identification, Preservation, and Collection** stages.\n\n---\n\n### Phase 1: Proactive Strategies (Before Litigation)\n\nThese are the most powerful cost-saving measures.\n\n1.  **Implement a Defensible Data Retention Policy**\n    *   **What it is:** A corporate policy that systematically and consistently defines how long different types of data are kept before being deleted.\n    *   **Why it works:** If data is deleted according to a routine, good-faith policy, it is not considered \u0022spoliation\u0022 when litigation arises. This significantly shrinks the universe of potentially relevant data.\n    *   **Key Action:** Classify data by type (e.g., financial records: 7 years; routine emails: 2 years; instant messages: 30 days) and enforce the policy automatically where possible.\n\n2.  **Leverage Advanced Communication Tools**\n    *   Use collaboration platforms like **Microsoft Teams, Slack, or Google Chat** with clear policies. These can often be set to auto-delete messages after a set period, unlike email which is kept forever by default.\n    *   **Caution:** These platforms are discoverable. Their use must be governed by your retention policy.\n\n3.  **Employee Training and Clean Culture**\n    *   Train employees not to use email as a filing cabinet.\n    *   Discourage \u0022Reply All\u0022 chains and massive attachments.\n    *   Encourage regular inbox cleanup.\n\n---\n\n### Phase 2: Early Case Assessment (ECA) \u0026 Strategic Scoping\n\nAs soon as a legal matter is reasonably anticipated, take control of the data landscape.\n\n1.  **Conduct a Custodian Interview Process**\n    *   **Identify Key Players:** Don\u0027t cast a wide net. Work with the legal team to identify the 10 most critical custodians, not 100.\n    *   **Question Custodians:** Ask them where they store relevant data (email, network drives, cloud storage, phones) and about their specific data habits. This helps you avoid collecting from irrelevant data sources.\n\n2.  **Define Tight Temporal and Subject Matter Boundaries**\n    *   **Date Ranges:** Negotiate the narrowest possible date range. A 3-month period is vastly cheaper than a 3-year period.\n    *   **Search Terms:** Develop a focused list of keywords and phrases with the legal team. Avoid single, generic terms like \u0022project\u0022 or \u0022financial.\u0022 Use Boolean logic (AND, OR, NOT) to create precise queries.\n        *   Example: \u0060(\u0022Project Phoenix\u0022 AND budget) NOT \u0022marketing\u0022\u0060\n\n3.  **Targeted Collection (Not \u0022Boiling the Ocean\u0022)**\n    *   **Custodian-Based:** Only collect from the identified key players.\n    *   **Repository-Based:** Only collect from relevant data sources (e.g., specific SharePoint sites, project folders). Avoid collecting entire network drives.\n    *   **Use Technology:** Employ tools that can perform a pre-collection \u0022cull\u0022 based on your defined criteria (dates, terms, file types).\n\n---\n\n### Phase 3: Technology-Assisted Review (TAR) and Advanced Culling\n\nOnce data is collected, use technology to aggressively reduce the volume before human review.\n\n1.  **DeNISTing \u0026 System File Removal**\n    *   **What it is:** Removing known application files (e.g., .exe, .dll) using the National Institute of Standards and Technology (NIST) list. These files are never relevant.\n    *   **Impact:** Can eliminate 5-15% of data volume instantly.\n\n2.  **Deduplication**\n    *   **What it is:** Identifying and removing duplicate files and emails across the entire dataset.\n    *   **Types:**\n        *   **Global Deduplication:** Removes duplicates across all custodians.\n        *   **Custodian Deduplication:** Keeps one copy per custodian.\n    *   **Impact:** This is one of the most effective steps, often reducing data by **20-40%**.\n\n3.  **Email Threading**\n    *   **What it is:** Identifying the most inclusive email in a thread (the one with all previous replies attached) and suppressing the earlier, incomplete emails.\n    *   **Impact:** Reviewers read one email instead of 10, cutting review time and cost significantly.\n\n4.  **Technology-Assisted Review (TAR)**\n    *   **What it is:** Using machine learning to prioritize or classify documents for review. A human reviewer codes a \u0022seed set\u0022 of documents, and the system predicts the relevance of the remaining documents.\n    *   **Models:**\n        *   **TAR 1.0 (Predictive Coding):** Excellent for separating relevant from non-relevant documents, allowing you to cull out large swaths of irrelevant data.\n        *   **TAR 2.0 (Continuous Active Learning - CAL):** Even more efficient, as the system continuously learns from every reviewer decision and prioritizes the most likely relevant documents next.\n    *   **Impact:** Proven to be more accurate and far faster than manual review, reducing the number of documents requiring human eyes.\n\n5.  **File Type Filtering**\n    *   **What it is:** Excluding irrelevant file types from processing and review.\n    *   **Examples:** You can safely filter out video files (.mp4, .avi), system files, and large program files if they are not relevant to the case.\n\n6.  **Domain Analysis \u0026 Communication Mapping**\n    *   **What it is:** Analyzing email traffic to identify the key players and their most frequent contacts. This can help you filter out communications with irrelevant external domains (e.g., \u0060newsletter@amazon.com\u0060).\n\n---\n\n### A Practical Workflow for Data Reduction\n\n| Step | Action | Estimated Reduction |\n| :--- | :--- | :--- |\n| **1. Pre-Collection** | Negotiate tight date ranges \u0026 custodian list. | **-20-50%** |\n| **2. Collection** | Use targeted, search-term focused collection. | **-10-30%** |\n| **3. Processing** | Apply DeNISTing, deduplication, and email threading. | **-30-60%** |\n| **4. ECA** | Use TAR and analytics to identify key documents and cull obvious noise. | **-40-80% of review set** |\n| **5. Review** | Leverage TAR and continuous learning to streamline the human review. | **-50%\u002B in review time** |\n\n### Key Takeaways for Success\n\n*   **Start Early:** The biggest savings happen before a single byte is collected.\n*   **Collaborate:** Work closely with your legal team, IT department, and eDiscovery vendor. Everyone must be aligned on the strategy.\n*   **Be Defensible:** Every decision to reduce data must be documented and based on a reasonable, justifiable strategy. You must be able to explain *why* you collected from A but not B, or why you used certain keywords.\n*   **Embrace Technology:** TAR and analytics are no longer \u0022nice-to-have.\u0022 They are essential tools for managing the scale of modern data.\n\nBy adopting this layered approach, you can transform an unmanageable, multi-terabyte data dump into a focused, relevant, and cost-effective document set for legal review.",
    "Gemini": "Reducing data volumes in eDiscovery is crucial for controlling costs, improving efficiency, and streamlining the review process. Here\u0027s a comprehensive overview of strategies and techniques for achieving this:\n\n**I. The EDRM Model and Data Reduction**\n\nThe Electronic Discovery Reference Model (EDRM) provides a framework for eDiscovery. Data reduction strategies are relevant across multiple stages:\n\n*   **Information Governance:**  Proper information governance policies are the most proactive way to reduce data volumes. Implement policies for:\n    *   **Data Retention Schedules:** Define how long different types of data need to be kept based on legal and business requirements. Regularly dispose of data that is no longer needed.\n    *   **Data Mapping:** Understand where critical information resides within the organization.\n    *   **Data Minimization:**  Reduce the amount of data created and stored.\n    *   **Employee Training:** Educate employees on data management best practices.\n*   **Identification:** Quickly locate and identify relevant data sources.\n*   **Preservation:**  Ensure that potentially relevant data is not altered or destroyed.\n*   **Collection:** Collect only the data that is reasonably likely to be relevant and proportional to the needs of the case.\n*   **Processing:**  This is a major phase for data reduction. Techniques employed include deduplication, deNISTing, email threading, filtering by date ranges, and keyword searches.\n*   **Review:** Reduced data volumes directly translate to faster and more cost-effective review.\n*   **Analysis:**  The reduced and processed data is easier to analyze.\n*   **Production:** Preparing the final data for production, including redacting and formatting.\n\n**II. Key Data Reduction Techniques**\n\nHere\u0027s a detailed breakdown of common data reduction techniques:\n\n**A. Early Case Assessment (ECA)**\n\n*   **Purpose:** Evaluate the merits of a case early on to determine if it\u0027s worth pursuing or to inform settlement negotiations.  ECA helps define the scope of discovery and identify key custodians and data sources.\n*   **Techniques:**\n    *   **Keyword Searching:**  Quickly identify and assess documents containing relevant keywords.\n    *   **Conceptual Search:** Find documents similar in concept or theme to known relevant documents.\n    *   **Custodian Interviews:**  Gather information from key custodians about data locations and document types.\n    *   **Data Sampling:**  Examine a representative sample of the data to understand its nature and relevance.\n    *   **Visualization Tools:** Use charts and graphs to identify patterns and trends in the data.\n\n**B. Data Deduplication**\n\n*   **Purpose:** Remove duplicate files across the entire data set.\n*   **Mechanism:**  Hashing algorithms (e.g., MD5, SHA-1) are used to create unique identifiers for each file.  Files with identical hashes are considered duplicates and only one copy is retained.\n*   **Types:**\n    *   **Global Deduplication:**  Removes duplicates across all custodians and data sources.\n    *   **Custodian-Level Deduplication:** Removes duplicates within each custodian\u0027s data.\n*   **Benefits:**  Significant reduction in data volume, especially in environments with shared files.\n*   **Considerations:**  Ensure the deduplication process is defensible. Document the methodology used.\n\n**C. DeNISTing (Operating System File Filtering)**\n\n*   **Purpose:** Exclude known system files, program files, and other non-relevant files from the data set.\n*   **Mechanism:** Compares file hashes against the National Institute of Standards and Technology (NIST) National Software Reference Library (NSRL) database. Files identified as known system files are excluded.\n*   **Benefits:**  Removes a large volume of irrelevant data, especially operating system files.\n*   **Considerations:**  Verify the accuracy and currency of the NIST database.  Consider whether to exclude *all* NIST files, as some might be relevant in certain cases (e.g., malware analysis).\n\n**D. Email Threading and Near-Duplicate Analysis**\n\n*   **Purpose:** Organize and group email messages into conversations (threads) and identify near-duplicate documents.\n*   **Email Threading:**\n    *   **Mechanism:** Analyzes email headers (To, From, Subject, Date, etc.) and content to identify email chains.  The most inclusive email in a thread (the \u0022inclusive email\u0022) contains all prior emails in the conversation.\n    *   **Benefits:** Reduces the number of emails requiring review by focusing on the inclusive email.  Allows reviewers to see the entire conversation history.\n    *   **Considerations:** Complex email threading algorithms are required for accuracy.\n*   **Near-Duplicate Analysis:**\n    *   **Mechanism:** Identifies documents that are substantially similar, even if not exact duplicates. Algorithms analyze text and metadata to determine similarity.\n    *   **Benefits:**  Reduces review effort by identifying and prioritizing the most relevant version of a near-duplicate document.\n    *   **Considerations:**  Set appropriate similarity thresholds to balance accuracy and efficiency.\n\n**E. Date Range Filtering**\n\n*   **Purpose:** Limit the data set to a specific date range relevant to the case.\n*   **Mechanism:**  Filters documents based on their creation date, modification date, or other date fields.\n*   **Benefits:**  Eliminates data outside the relevant timeframe.\n*   **Considerations:**  Carefully define the date range based on the facts of the case.\n\n**F. Keyword Searching and Filtering**\n\n*   **Purpose:**  Identify and extract documents containing specific keywords or terms relevant to the case.\n*   **Mechanism:**  Uses search algorithms to locate documents containing the specified keywords.\n*   **Benefits:**  Narrows the focus to the most relevant documents.\n*   **Considerations:**  Develop a comprehensive keyword list in consultation with legal counsel and subject matter experts.  Consider using Boolean operators (AND, OR, NOT) and proximity searches to refine search results.\n\n**G. File Type Filtering**\n\n*   **Purpose:** Exclude certain file types that are unlikely to be relevant (e.g., system files, program files, image files, audio files).\n*   **Mechanism:** Filters documents based on their file extension (e.g., .exe, .dll, .jpg, .mp3).\n*   **Benefits:**  Removes non-essential data.\n*   **Considerations:**  Be cautious about excluding file types, as they may contain relevant information in some cases (e.g., embedded images in documents).\n\n**H. Custodian Filtering**\n\n*   **Purpose:**  Focus collection and processing on data from specific custodians identified as key players in the case.\n*   **Mechanism:**  Collect data only from designated custodians.\n*   **Benefits:**  Reduces the volume of data to be processed and reviewed.\n*   **Considerations:**  Carefully select custodians based on the facts of the case and legal requirements.\n\n**I. Managed Early Data Assessment (MEDA)**\n\n*   **Purpose:** Using technology assisted review tools earlier in the eDiscovery process to assess the relevance of documents before collection and processing.\n*   **Mechanism:** This is done by using TAR tools to sample, search, and analyze the data, helping legal teams make informed decisions about which data to collect and process.\n*   **Benefits:** Significant cost savings by reducing the amount of data that needs to be collected, processed, and reviewed.\n*   **Considerations:**  Requires expertise in TAR and a well-defined workflow to ensure accuracy and defensibility.\n\n**J. Technology Assisted Review (TAR) / Predictive Coding**\n\n*   **Purpose:**  Use machine learning algorithms to prioritize and classify documents based on their relevance to the case.\n*   **Mechanism:**  A subject matter expert (SME) or attorney manually reviews a sample of documents and identifies those that are relevant (\u0022seed set\u0022). The TAR system learns from this training set and then predicts the relevance of the remaining documents.\n*   **Benefits:**  Significantly reduces the number of documents requiring manual review.  Improves the accuracy and consistency of review.\n*   **Considerations:**  Requires a well-defined workflow, a representative training set, and ongoing monitoring of the system\u0027s performance.  Requires expertise in TAR.\n\n**III. Strategic Considerations and Best Practices**\n\n*   **Proportionality:**  Ensure that the scope of discovery is proportional to the needs of the case.\n*   **Defensibility:**  Document all data reduction techniques used and ensure that the process is defensible in court.\n*   **Transparency:**  Communicate with opposing counsel about the data reduction techniques being used.\n*   **Collaboration:**  Involve legal counsel, IT professionals, and eDiscovery experts in the data reduction process.\n*   **Testing and Validation:**  Thoroughly test and validate data reduction techniques to ensure accuracy and completeness.\n*   **Software Selection:**  Choose eDiscovery software with robust data reduction capabilities.  Ensure the software is compatible with the organization\u0027s IT infrastructure and data sources.\n*   **Ongoing Monitoring:**  Monitor the effectiveness of data reduction techniques and make adjustments as needed.\n*   **Legal Hold:**  Implement a legal hold policy to prevent the destruction of potentially relevant data.  Suspend data retention schedules for custodians involved in litigation.\n*   **Data Security:** Implement appropriate security measures to protect data during collection, processing, and review.\n*   **Iterative Approach:** Data reduction is not a one-time process.  It is an iterative process that should be revisited throughout the eDiscovery lifecycle. As you learn more about the case, you may need to refine your data reduction strategies.\n\n**IV.  The Impact of Data Volume on Costs**\n\nReducing data volumes directly translates into cost savings in several areas:\n\n*   **Collection:** Less data to collect means lower collection costs.\n*   **Processing:** Reduced processing costs due to less data to process.\n*   **Storage:** Lower storage costs due to smaller data sets.\n*   **Review:**  The largest cost component of eDiscovery. Less data to review significantly reduces review costs.\n*   **Production:**  Lower production costs due to less data to produce.\n\n**V.  Examples of Cost Savings**\n\n*   **Deduplication:**  Deduplication can often reduce data volumes by 20-50%, resulting in significant cost savings in processing and review.\n*   **DeNISTing:**  DeNISTing can remove up to 30% of the data, further reducing processing and storage costs.\n*   **Technology Assisted Review (TAR):** TAR can significantly reduce the number of documents requiring manual review, often by 50-90%, resulting in substantial cost savings.\n\nBy implementing these strategies, organizations can effectively reduce data volumes in eDiscovery, control costs, improve efficiency, and streamline the review process. Remember to always prioritize defensibility and proportionality in all data reduction efforts.\n"
  }'
  published-date="21:38"
/>

## Analysis

This comparison demonstrates the different approaches each AI model takes when responding to the same prompt. Here are the key differences observed:

### Response Characteristics

**ChatGPT**: Provides a direct response with 36 sentences. 

**DeepSeek**: Provides a direct response with 96 sentences. 

**Gemini**: Provides a direct response with 130 sentences. 

## Key Takeaways

- Each model brings unique strengths to this type of query
- Response styles vary significantly between models
- Consider your specific use case when choosing between these models

## Try This Comparison Yourself

Want to test these models with your own prompts? Visit [SNEOS.com](https://sneos.com) to compare AI responses side-by-side in real-time.

---

*This comparison was generated using the [SNEOS AI Comparison Tool](https://sneos.com)*
*Published: October 01, 2025 | Models: ChatGPT, DeepSeek, Gemini*
