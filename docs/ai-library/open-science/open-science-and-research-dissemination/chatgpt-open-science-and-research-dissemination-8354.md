---
title: "I need to engage with open science as reviewer or editor. Help me: 1) Request data and code when reviewing"
description: "AI model comparison: I need to engage with open science as reviewer or editor. Help me: 1) Request data and code when rev..."
date: 2025-10-15
category: comparison
layout: Clean
tags:
  - ChatGPT
  - AI Comparison
author: SNEOS Community
head:
  - - meta
    - name: description
      content: "AI model comparison: I need to engage with open science as reviewer or editor. Help me: 1) Request data and code when reviewing..."
---

<ChatComparison
  title="I need to engage with open science as reviewer or editor. Help me: 1) Request data and code when reviewing"
  prompt="I need to engage with open science as reviewer or editor. Help me: 1) Request data and code when reviewing"
  :responses='{"ChatGPT": "Engaging with open science as a reviewer or editor is an excellent way to promote transparency, reproducibility, and collaboration in research. Requesting data and code when reviewing a manuscript is a key part of this engagement. Here\u2019s how you can effectively incorporate this practice into your reviewing process:\n\n### 1. **Include a Statement in Your Review Request**\n\nWhen you receive a manuscript to review, you can include a statement in your review request or in your initial comments if you are in a position to do so (e.g., if you are the editor). Here\u2019s an example statement you can use:\n\n\u003E \u0022As part of our commitment to open science, I encourage the authors to share their raw data and code used for analyses. This will enable proper verification of the results and contribute to the transparency of the research process.\u0022\n\n### 2. **Incorporate Open Science Principles in Your Feedback**\n\nWhen providing feedback on a manuscript, emphasize the importance of data and code sharing. You can suggest:\n\n- **Data Availability Statement**: Recommend that authors provide a clear data availability statement, detailing where the data can be accessed (e.g., a public repository) or if there are any restrictions.\n  \n- **Code Sharing**: Suggest that authors share the code used for data analysis, ideally in a public repository like GitHub, GitLab, or a preprint server.\n  \n- **Documentation**: Encourage the authors to document their data and code adequately, making it easier for others to understand and replicate their work.\n\n### 3. **Use Review Guidelines**\n\nIf you are part of a journal or organization with specific guidelines on open science, make sure to refer to those. You can consult the journal\u2019s policies regarding data sharing and mention those in your review or editorial comments.\n\n### 4. **Educate on Best Practices**\n\nIf you\u0027re reviewing a manuscript, you might find it useful to provide resources or links to best practices regarding data and code sharing. This can help authors understand the importance of these points:\n\n- **FAIR Principles**: Mention the FAIR data principles (Findable, Accessible, Interoperable, and Reusable) as a framework for sharing their data.\n  \n- **Data Repositories**: Suggest appropriate data repositories based on the field of study (e.g., Dryad, Zenodo, Figshare) where they can deposit their data securely.\n\n### 5. **Encourage Open Research Practices**\n\nIn addition to requesting data and code, you can encourage the authors to consider open practices more broadly, such as:\n\n- Pre-registering their study and hypotheses.\n- Sharing preprints before peer review.\n- Collaborating openly with others in their field.\n\n### 6. **Follow Up on Data Requests**\n\nIf the authors provide their data and code in response to your review, consider following up to confirm that they are adequately documented and accessible. Provide constructive feedback on how authors can improve the sharing process.\n\n### Example Request to Authors in Your Review:\n\nIn your review comments, you could write:\n\n\u003E \u0022To enhance the transparency and reproducibility of your study, I recommend that you provide access to the raw data and code used in your analyses. Please consider depositing these in a public repository, such as [insert appropriate repositories], and include a data availability statement in your manuscript to direct readers to the resources.\u0022\n\nBy adopting these practices, you\u0027ll actively contribute to the open science movement as a reviewer or editor, fostering a culture that values transparency, collaboration, and reproducibility in research."}' published-date="{DateTime.UtcNow:HH:mm}"
/>

## Analysis

This comparison demonstrates the different approaches each AI model takes when responding to the same prompt. Here are the key differences observed:

### Response Characteristics

**ChatGPT**: Provides a detailed response with 34 sentences. 

## Key Takeaways

- Each model brings unique strengths to this type of query
- Response styles vary significantly between models
- Consider your specific use case when choosing between these models

## Try This Comparison Yourself

Want to test these models with your own prompts? Visit [SNEOS.com](https://sneos.com) to compare AI responses side-by-side in real-time.

---

*This comparison was generated using the [SNEOS AI Comparison Tool](https://sneos.com)*
*Published: October 15, 2025 | Models: ChatGPT*
