---
title: "Analyze survey data from 300 participants on student anxiety levels. Help me: 1) Clean and prepare data (handle missing values, outliers), 2) Run appropriate statistical tests (t-tests, ANOVA, chi-square), 3) Check assumptions for each test, 4) Calculate effect sizes and confidence intervals, 5) Create publication-ready tables in APA format. Use R or Python."
description: "AI model comparison: Analyze survey data from 300 participants on student anxiety levels. Help me: 1) Clean and prepare d..."
date: 2025-10-15
category: comparison
layout: Clean
tags:
  - ChatGPT
  - Grok
  - AI Comparison
author: SNEOS Community
head:
  - - meta
    - name: description
      content: "AI model comparison: Analyze survey data from 300 participants on student anxiety levels. Help me: 1) Clean and prepare data (handle missing ..."
---

<ChatComparison
  title="Analyze survey data from 300 participants on student anxiety levels. Help me: 1) Clean and prepare data (handle missing values, outliers), 2) Run appropriate statistical tests (t-tests, ANOVA, chi-square), 3) Check assumptions for each test, 4) Calculate effect sizes and confidence intervals, 5) Create publication-ready tables in APA format. Use R or Python."
  prompt="Analyze survey data from 300 participants on student anxiety levels. Help me: 1) Clean and prepare data (handle missing values, outliers), 2) Run appropriate statistical tests (t-tests, ANOVA, chi-square), 3) Check assumptions for each test, 4) Calculate effect sizes and confidence intervals, 5) Create publication-ready tables in APA format. Use R or Python."
  :responses='{"ChatGPT": "Analyzing survey data on student anxiety levels involves several detailed steps. Below, I will outline a procedure using Python, including data cleaning, statistical tests, assumption checks, effect size calculations, and creation of APA-style tables. You\u0027ll need to adjust certain parts based on your specific data and research questions.\n\n### Step 1: Clean and Prepare Data\n\nFirst, we need to load and clean the data. Let\u0027s assume your data is in a CSV format. We\u0027ll use pandas for data manipulation.\n\n\u0060\u0060\u0060python\nimport pandas as pd\nimport numpy as np\n\n# Load the dataset\ndata = pd.read_csv(\u0022student_anxiety_data.csv\u0022)\n\n# Display the first few rows\nprint(data.head())\n\n# Check for missing values\nprint(data.isnull().sum())\n\n# Handle missing values (e.g., drop or fill)\ndata.fillna(data.mean(), inplace=True)  # Example: fill missing numerical values with the mean\n# Alternatively, you might drop missing rows\n# data.dropna(inplace=True)\n\n# Identify outliers (using IQR method for example)\nQ1 = data[\u0027anxiety_level\u0027].quantile(0.25)\nQ3 = data[\u0027anxiety_level\u0027].quantile(0.75)\nIQR = Q3 - Q1\n\n# Define outlier bounds\nlower_bound = Q1 - 1.5 * IQR\nupper_bound = Q3 \u002B 1.5 * IQR\ndata = data[(data[\u0027anxiety_level\u0027] \u003E= lower_bound) \u0026 (data[\u0027anxiety_level\u0027] \u003C= upper_bound)]\n\n# Check cleaned data\nprint(data.describe())\n\u0060\u0060\u0060\n\n### Step 2: Run Appropriate Statistical Tests\n\nNow, assuming you have a categorical variable (e.g., \u0027group\u0027 with levels \u0027A\u0027 and \u0027B\u0027) and a continuous variable (e.g., anxiety levels).\n\n#### T-Test\n\n\u0060\u0060\u0060python\nfrom scipy import stats\n\ngroup_a = data[data[\u0027group\u0027] == \u0027A\u0027][\u0027anxiety_level\u0027]\ngroup_b = data[data[\u0027group\u0027] == \u0027B\u0027][\u0027anxiety_level\u0027]\n\nt_stat, p_value = stats.ttest_ind(group_a, group_b)\nprint(\u0022T-test result: t-statistic =\u0022, t_stat, \u0022; p-value =\u0022, p_value)\n\u0060\u0060\u0060\n\n#### ANOVA (if there are more than two groups)\n\n\u0060\u0060\u0060python\nanova_result = stats.f_oneway(*(data[data[\u0027group\u0027] == group][\u0027anxiety_level\u0027] for group in data[\u0027group\u0027].unique()))\nprint(\u0022ANOVA result: F-statistic =\u0022, anova_result.statistic, \u0022; p-value =\u0022, anova_result.pvalue)\n\u0060\u0060\u0060\n\n#### Chi-Square Test\n\nAssuming you have categorical data on anxiety levels, you could run a chi-square test as follows:\n\n\u0060\u0060\u0060python\ncontingency_table = pd.crosstab(data[\u0027anxiety_category\u0027], data[\u0027group\u0027])\nchi2, p, dof, expected = stats.chi2_contingency(contingency_table)\nprint(\u0022Chi-Square Test: chi2 =\u0022, chi2, \u0022; p-value =\u0022, p)\n\u0060\u0060\u0060\n\n### Step 3: Check Assumptions\n\nFor t-tests and ANOVA:\n\n1. **Normality**: Use the Shapiro-Wilk test.\n2. **Equality of Variances**: Use Levene\u2019s Test.\n\n\u0060\u0060\u0060python\n# Normality test\nshapiro_a = stats.shapiro(group_a)\nshapiro_b = stats.shapiro(group_b)\nprint(\u0022Shapiro-Wilk test for group A:\u0022, shapiro_a)\nprint(\u0022Shapiro-Wilk test for group B:\u0022, shapiro_b)\n\n# Levene\u0027s test\nlevene_test = stats.levene(group_a, group_b)\nprint(\u0022Levene\u0027s test result:\u0022, levene_test)\n\u0060\u0060\u0060\n\nFor Chi-Square, you should have a sufficient sample size (\u003E5 in each category).\n\n### Step 4: Calculate Effect Sizes and Confidence Intervals\n\nFor t-tests, use Cohen\u0027s d; for ANOVA, use eta squared; and for chi-square, use Cram\u00E9r\u0027s V.\n\n\u0060\u0060\u0060python\n# Cohen\u0027s d for t-test\ncohen_d = (np.mean(group_a) - np.mean(group_b)) / np.sqrt((np.std(group_a)**2 \u002B np.std(group_b)**2) / 2)\nprint(\u0022Cohen\u0027s d:\u0022, cohen_d)\n\n# Confidence intervals\nconf_int_a = stats.t.interval(0.95, len(group_a)-1, loc=np.mean(group_a), scale=stats.sem(group_a))\nconf_int_b = stats.t.interval(0.95, len(group_b)-1, loc=np.mean(group_b), scale=stats.sem(group_b))\nprint(\u002295% CI for Group A:\u0022, conf_int_a)\nprint(\u002295% CI for Group B:\u0022, conf_int_b)\n\u0060\u0060\u0060\n\n### Step 5: Create APA Style Tables\n\nWe can use \u0060pandas\u0060 with \u0060dataframe.to_latex()\u0060 for publication-ready tables.\n\n\u0060\u0060\u0060python\n# Create a summary table\nsummary_table = pd.DataFrame({\n    \u0027Group\u0027: [\u0027A\u0027, \u0027B\u0027],\n    \u0027Mean Anxiety Level\u0027: [np.mean(group_a), np.mean(group_b)],\n    \u0027Standard Deviation\u0027: [np.std(group_a), np.std(group_b)],\n    \u0027Sample Size\u0027: [len(group_a), len(group_b)],\n    \u0027Cohen\\\u0027s d\u0027: [cohen_d, \u0027\u0027], # This applies only to comparison\n})\n\n# Save to LaTeX\nsummary_table.to_latex(\u0027summary_table.tex\u0027, index=False)\n\u0060\u0060\u0060\n\nReplace paths and variable names according to your specific dataset.\n\n### Note:\n- Be sure to adjust the assumptions checks, effect size calculations, and table formats according to your actual research question.\n- If you prefer R instead of Python, similar methodologies apply, using \u0060dplyr\u0060, \u0060ggplot2\u0060, \u0060stats\u0060, and \u0060knitr\u0060 packages for data management and reporting.", "Grok": "I\u0027ll help you analyze the survey data on student anxiety levels using Python. Since you haven\u0027t provided the raw data, I\u0027ll first create a realistic dataset based on common anxiety survey scenarios and then perform a complete analysis. Here\u0027s a comprehensive solution:\n\n\u0060\u0060\u0060python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nfrom scipy.stats import shapiro, levene\nimport statsmodels.api as sm\nfrom statsmodels.stats.anova import anova_lm\nfrom statsmodels.stats.multicomp import pairwise_tukeyhsd\nfrom statsmodels.formula.api import ols\nimport warnings\nwarnings.filterwarnings(\u0027ignore\u0027)\n\n# Set style for publication-ready plots\nplt.style.use(\u0027seaborn-v0_8\u0027)\nsns.set_palette(\u0022colorblind\u0022)\n\n# Create realistic survey data for 300 participants\nnp.random.seed(42)  # For reproducibility\n\nn = 300\ndata = {\n    \u0027participant_id\u0027: range(1, n\u002B1),\n    \u0027age\u0027: np.random.normal(20, 2, n).astype(int),\n    \u0027gender\u0027: np.random.choice([\u0027Male\u0027, \u0027Female\u0027, \u0027Non-binary\u0027], n, p=[0.48, 0.48, 0.04]),\n    \u0027year\u0027: np.random.choice([\u0027Freshman\u0027, \u0027Sophomore\u0027, \u0027Junior\u0027, \u0027Senior\u0027], n, p=[0.25, 0.25, 0.25, 0.25]),\n    \u0027gpa\u0027: np.clip(np.random.normal(3.2, 0.5, n), 0, 4),\n    \n    # Anxiety measures (GAD-7 scale: 0-21, higher = more anxiety)\n    \u0027anxiety_score\u0027: np.random.normal(10, 4, n),\n    \u0027sleep_quality\u0027: np.random.choice([1, 2, 3, 4, 5], n, p=[0.1, 0.15, 0.3, 0.25, 0.2]),  # 1=poor, 5=excellent\n    \n    # Categorical variables for additional analysis\n    \u0027major_stress\u0027: np.random.choice([\u0027Low\u0027, \u0027Moderate\u0027, \u0027High\u0027], n, p=[0.3, 0.4, 0.3]),\n    \u0027academic_load\u0027: np.random.choice([\u0027Light\u0027, \u0027Moderate\u0027, \u0027Heavy\u0027], n, p=[0.2, 0.5, 0.3]),\n    \u0027social_support\u0027: np.random.choice([\u0027Low\u0027, \u0027Moderate\u0027, \u0027High\u0027], n, p=[0.25, 0.5, 0.25])\n}\n\n# Create the dataset\ndf = pd.DataFrame(data)\n\n# Introduce some realistic missing values and outliers\n# Missing data (5% missing completely at random)\nmask_missing = np.random.random(n) \u003C 0.05\ndf.loc[mask_missing, [\u0027anxiety_score\u0027, \u0027gpa\u0027, \u0027sleep_quality\u0027]] = np.nan\n\n# Add some outliers for anxiety scores (beyond reasonable range)\noutlier_indices = np.random.choice(df.index, 15, replace=False)\ndf.loc[outlier_indices[:5], \u0027anxiety_score\u0027] = np.random.uniform(25, 35, 5)  # High outliers\ndf.loc[outlier_indices[5:10], \u0027anxiety_score\u0027] = np.random.uniform(-5, -1, 5)  # Low outliers\n\nprint(\u0022Dataset created with 300 participants\u0022)\nprint(f\u0022Initial shape: {df.shape}\u0022)\nprint(\u0022\\nFirst few rows:\u0022)\nprint(df.head())\n\n# =============================================================================\n# 1. DATA CLEANING AND PREPARATION\n# =============================================================================\n\ndef clean_data(df):\n    \u0022\u0022\u0022Clean and prepare the dataset\u0022\u0022\u0022\n    df_clean = df.copy()\n    \n    print(\u0022\\n\u0022 \u002B \u0022=\u0022*60)\n    print(\u00221. DATA CLEANING AND PREPARATION\u0022)\n    print(\u0022=\u0022*60)\n    \n    # Remove duplicate participants\n    initial_n = len(df_clean)\n    df_clean = df_clean.drop_duplicates(subset=[\u0027participant_id\u0027], keep=\u0027first\u0027)\n    print(f\u0022Removed {initial_n - len(df_clean)} duplicates\u0022)\n    \n    # Handle outliers in anxiety scores using IQR method\n    Q1 = df_clean[\u0027anxiety_score\u0027].quantile(0.25)\n    Q3 = df_clean[\u0027anxiety_score\u0027].quantile(0.75)\n    IQR = Q3 - Q1\n    lower_bound = Q1 - 1.5 * IQR\n    upper_bound = Q3 \u002B 1.5 * IQR\n    \n    # Clip outliers instead of removing (more conservative)\n    n_outliers_before = ((df_clean[\u0027anxiety_score\u0027] \u003C lower_bound) | \n                        (df_clean[\u0027anxiety_score\u0027] \u003E upper_bound)).sum()\n    \n    df_clean[\u0027anxiety_score\u0027] = df_clean[\u0027anxiety_score\u0027].clip(lower=0, upper=21)  # GAD-7 max=21\n    df_clean[\u0027anxiety_score\u0027] = np.clip(df_clean[\u0027anxiety_score\u0027], lower_bound, upper_bound)\n    \n    print(f\u0022Clipped {n_outliers_before} outliers in anxiety scores (IQR method)\u0022)\n    print(f\u0022Anxiety score range after cleaning: {df_clean[\u0027anxiety_score\u0027].min():.1f} - {df_clean[\u0027anxiety_score\u0027].max():.1f}\u0022)\n    \n    # Handle age outliers\n    df_clean = df_clean[(df_clean[\u0027age\u0027] \u003E= 16) \u0026 (df_clean[\u0027age\u0027] \u003C= 30)]\n    print(f\u0022Filtered age outliers: kept participants aged 16-30 ({len(df_clean)} remaining)\u0022)\n    \n    # Handle missing values\n    print(\u0022\\nMissing value summary:\u0022)\n    missing_summary = df_clean.isnull().sum()\n    print(missing_summary[missing_summary \u003E 0])\n    \n    # Impute continuous variables with median, categorical with mode\n    for col in [\u0027anxiety_score\u0027, \u0027gpa\u0027]:\n        if col in df_clean.columns:\n            median_val = df_clean[col].median()\n            n_missing = df_clean[col].isnull().sum()\n            df_clean[col] = df_clean[col].fillna(median_val)\n            print(f\u0022  Imputed {n_missing} missing {col} values with median ({median_val:.1f})\u0022)\n    \n    for col in [\u0027sleep_quality\u0027, \u0027gender\u0027, \u0027year\u0027]:\n        if col in df_clean.columns:\n            mode_val = df_clean[col].mode()[0]\n            n_missing = df_clean[col].isnull().sum()\n            df_clean[col] = df_clean[col].fillna(mode_val)\n            print(f\u0022  Imputed {n_missing} missing {col} values with mode ({mode_val})\u0022)\n    \n    # Convert sleep_quality to numeric if it\u0027s categorical\n    if df_clean[\u0027sleep_quality\u0027].dtype == \u0027object\u0027:\n        df_clean[\u0027sleep_quality\u0027] = pd.to_numeric(df_clean[\u0027sleep_quality\u0027], errors=\u0027coerce\u0027)\n    \n    # Create anxiety categories for chi-square analysis\n    df_clean[\u0027anxiety_category\u0027] = pd.cut(df_clean[\u0027anxiety_score\u0027], \n                                        bins=[-1, 5, 10, 15, 21], \n                                        labels=[\u0027Minimal\u0027, \u0027Mild\u0027, \u0027Moderate\u0027, \u0027Severe\u0027])\n    \n    print(f\u0022\\nFinal cleaned dataset: {len(df_clean)} participants\u0022)\n    print(f\u0022Missing values after cleaning: {df_clean.isnull().sum().sum()}\u0022)\n    \n    return df_clean\n\n# Clean the data\ndf_clean = clean_data(df)\n\n# =============================================================================\n# 2. DESCRIPTIVE STATISTICS\n# =============================================================================\n\ndef descriptive_stats(df):\n    \u0022\u0022\u0022Generate descriptive statistics\u0022\u0022\u0022\n    print(\u0022\\n\u0022 \u002B \u0022=\u0022*60)\n    print(\u00222. DESCRIPTIVE STATISTICS\u0022)\n    print(\u0022=\u0022*60)\n    \n    # Overall descriptive statistics\n    print(\u0022\\nAnxiety Score Descriptive Statistics:\u0022)\n    desc_stats = df[[\u0027anxiety_score\u0027, \u0027age\u0027, \u0027gpa\u0027, \u0027sleep_quality\u0027]].describe()\n    print(desc_stats.round(2))\n    \n    # Categorical variable frequencies\n    print(\u0022\\nCategorical Variable Frequencies:\u0022)\n    cat_vars = [\u0027gender\u0027, \u0027year\u0027, \u0027major_stress\u0027, \u0027academic_load\u0027, \u0027social_support\u0027, \u0027anxiety_category\u0027]\n    for var in cat_vars:\n        if var in df.columns:\n            print(f\u0022\\n{var}:\u0022)\n            print(df[var].value_counts().sort_index() if df[var].dtype == \u0027object\u0027 else \n                  df[var].value_counts().sort_index())\n\n# Generate descriptive statistics\ndescriptive_stats(df_clean)\n\n# =============================================================================\n# 3. STATISTICAL TESTS WITH ASSUMPTION CHECKING\n# =============================================================================\n\ndef run_statistical_tests(df):\n    \u0022\u0022\u0022Run appropriate statistical tests with assumption checking\u0022\u0022\u0022\n    print(\u0022\\n\u0022 \u002B \u0022=\u0022*60)\n    print(\u00223. STATISTICAL TESTS WITH ASSUMPTION CHECKING\u0022)\n    print(\u0022=\u0022*60)\n    \n    results = {}\n    \n    # 3.1 Independent t-test: Gender differences in anxiety\n    print(\u0022\\n3.1 INDEPENDENT T-TEST: Gender differences in anxiety\u0022)\n    \n    # Prepare data (exclude Non-binary due to small sample)\n    df_ttest = df[df[\u0027gender\u0027].isin([\u0027Male\u0027, \u0027Female\u0027])].copy()\n    \n    male_anxiety = df_ttest[df_ttest[\u0027gender\u0027] == \u0027Male\u0027][\u0027anxiety_score\u0027]\n    female_anxiety = df_ttest[df_ttest[\u0027gender\u0027] == \u0027Female\u0027][\u0027anxiety_score\u0027]\n    \n    # Assumption 1: Normality (Shapiro-Wilk test)\n    print(f\u0022\\nAssumption Checking:\u0022)\n    shapiro_male = shapiro(male_anxiety.dropna())\n    shapiro_female = shapiro(female_anxiety.dropna())\n    print(f\u0022Shapiro-Wilk normality test - Male: W={shapiro_male.statistic:.3f}, p={shapiro_male.pvalue:.3f}\u0022)\n    print(f\u0022Shapiro-Wilk normality test - Female: W={"}' published-date="{DateTime.UtcNow:HH:mm}"
/>

## Analysis

This comparison demonstrates the different approaches each AI model takes when responding to the same prompt. Here are the key differences observed:

### Response Characteristics

**ChatGPT**: Provides a direct response with 71 sentences. 

**Grok**: Provides a direct response with 135 sentences. 

## Key Takeaways

- Each model brings unique strengths to this type of query
- Response styles vary significantly between models
- Consider your specific use case when choosing between these models

## Try This Comparison Yourself

Want to test these models with your own prompts? Visit [SNEOS.com](https://sneos.com) to compare AI responses side-by-side in real-time.

---

*This comparison was generated using the [SNEOS AI Comparison Tool](https://sneos.com)*
*Published: October 15, 2025 | Models: ChatGPT, Grok*
