---
title: "I'm planning a living systematic review on COVID-19 vaccine effectiveness that will be continuously updated. Guide me: 1) Set up infrastructure for ongoing searches and screening"
description: "AI model comparison: I'm planning a living systematic review on COVID-19 vaccine effectiveness that will be continuously ..."
date: 2025-10-15
category: comparison
layout: Clean
tags:
  - Claude
  - Gemini
  - Grok
  - AI Comparison
author: SNEOS Community
head:
  - - meta
    - name: description
      content: "AI model comparison: I'm planning a living systematic review on COVID-19 vaccine effectiveness that will be continuously updated. Guide me: 1..."
---

<ChatComparison
  title="I'm planning a living systematic review on COVID-19 vaccine effectiveness that will be continuously updated. Guide me: 1) Set up infrastructure for ongoing searches and screening"
  prompt="I'm planning a living systematic review on COVID-19 vaccine effectiveness that will be continuously updated. Guide me: 1) Set up infrastructure for ongoing searches and screening"
  :responses='{"Claude": "# Infrastructure Setup for a Living Systematic Review on COVID-19 Vaccine Effectiveness\n\n## 1. Search Infrastructure\n\n### A. Automated Database Searches\n\n**Set up automated alerts in:**\n- **PubMed/MEDLINE**: Create saved searches with email alerts (daily/weekly)\n- **Embase**: Set up auto-alerts via Ovid or Embase.com\n- **Cochrane COVID-19 Study Register**: Subscribe to updates\n- **WHO COVID-19 Database**: Regular monitoring\n- **medRxiv/bioRxiv**: RSS feeds for preprints\n- **ClinicalTrials.gov**: Saved searches for new/updated trials\n\n**Technical implementation:**\n\u0060\u0060\u0060\n- Use IFTTT or Zapier to aggregate alerts\n- Create a dedicated email folder/label\n- Set up calendar reminders for manual checks\n\u0060\u0060\u0060\n\n### B. Citation Management System\n\n**Recommended setup:**\n- **Covidence** or **DistillerSR** (designed for living reviews)\n  - Allows continuous import\n  - Tracks updates automatically\n  - Facilitates team collaboration\n  \nAlternative: **Rayyan** (free, good for continuous screening)\n\n**Configuration:**\n- Create project with standardized search strings\n- Enable deduplication algorithms\n- Set up user permissions for team members\n\n### C. Grey Literature \u0026 Regulatory Monitoring\n\n**Establish monitoring for:**\n- FDA, EMA, Health Canada, TGA (Australia) vaccine briefings\n- CDC MMWR reports\n- National immunization technical advisory groups (NITAGs)\n- Manufacturer press releases (Pfizer, Moderna, etc.)\n\n**Use:**\n- Google Scholar alerts for key terms\n- RSS feeds from agency websites\n- Social media monitoring (Twitter lists of key researchers)\n\n---\n\n## 2. Screening Infrastructure\n\n### A. Two-Stage Screening Process\n\n**Stage 1: Title/Abstract Screening**\n- Dual independent screening for all new records\n- Use screening software with:\n  - Conflict resolution workflows\n  - Progress tracking\n  - Inter-rater reliability calculations\n\n**Stage 2: Full-Text Screening**\n- Structured form with explicit inclusion/exclusion criteria\n- Documentation of exclusion reasons\n- Version control for screening forms\n\n### B. Machine Learning Integration (Optional but Recommended)\n\n**Tools to consider:**\n- **ASReview** (free, open-source active learning)\n- **Abstrackr**\n- **EPPI-Reviewer** (has machine learning features)\n\n**Benefits:**\n- Prioritizes relevant studies\n- Reduces screening burden over time\n- Maintains human oversight\n\n### C. Standardized Screening Forms\n\n**Create detailed forms including:**\n\n\u0060\u0060\u0060\nINCLUSION CRITERIA:\n\u2610 Population: Any age group receiving COVID-19 vaccine\n\u2610 Intervention: Any approved/investigational COVID-19 vaccine\n\u2610 Comparator: Unvaccinated, placebo, or other vaccine\n\u2610 Outcomes: \n  - Infection (PCR/RAT confirmed)\n  - Symptomatic disease\n  - Hospitalization\n  - ICU admission\n  - Death\n  - By variant (Alpha, Delta, Omicron, etc.)\n\u2610 Study design: RCTs, cohort, case-control, test-negative design\n\u2610 Minimum follow-up: [Define threshold]\n\nEXCLUSION CRITERIA:\n\u2610 Immunogenicity-only studies\n\u2610 Commentaries, editorials without original data\n\u2610 Case reports/series \u003C[X] participants\n\u2610 Non-human studies\n\u0060\u0060\u0060\n\n---\n\n## 3. Team Structure \u0026 Workflow\n\n### A. Core Team Roles\n\n**Minimum team:**\n- **Project lead** (1): Overall coordination, updates schedule\n- **Information specialist** (1): Search strategy management\n- **Screeners** (2-3): Independent screening with rotation\n- **Data extractors** (2): Quality assessment and extraction\n- **Statistician** (1): Meta-analysis updates\n- **Clinical expert** (1): Interpretation and context\n\n### B. Update Frequency\n\n**Decision framework:**\n\u0060\u0060\u0060\nHigh frequency (monthly): First 6-12 months or during:\n- Emergence of new variants\n- New vaccines approved\n- Major policy changes\n\nModerate frequency (quarterly): Stabilization phase\n\nLow frequency (bi-annually): Mature evidence base\n\nTriggers for ad-hoc updates:\n- Major new RCT published\n-", "Gemini": "A living systematic review (LSR) on COVID-19 vaccine effectiveness is an ambitious and highly valuable undertaking, given the rapidly evolving evidence base. Setting up the infrastructure for continuous updates is paramount to its success. Here\u2019s a comprehensive guide:\n\n---\n\n## Setting Up Infrastructure for an Ongoing Living Systematic Review (LSR) on COVID-19 Vaccine Effectiveness\n\n### Phase 1: Foundation \u2013 Initial Setup \u0026 Protocol Development\n\nBefore continuous updates, you need a robust initial setup.\n\n1.  **Develop a Comprehensive Protocol:**\n    *   Register your protocol (e.g., PROSPERO). Clearly state it\u0027s an LSR.\n    *   **Research Question(s):** PICO format (e.g., \u0022What is the effectiveness of COVID-19 vaccines against symptomatic infection, severe disease, and mortality in different populations and against different variants?\u0022).\n    *   **Inclusion/Exclusion Criteria:** Detail populations, interventions (specific vaccines/doses), comparators (unvaccinated, placebo, other vaccines), outcomes (infection, hospitalization, death, transmission, adverse events), study designs (RCTs, observational studies, test-negative designs), language, and publication dates (continuously updated).\n    *   **Update Frequency:** Define the target frequency for searches, screening, data extraction, and synthesis (e.g., weekly, bi-weekly, monthly).\n    *   **Reporting Strategy:** How will updates be published? (e.g., dedicated website, cumulative manuscript, specific journal format).\n\n2.  **Team Formation \u0026 Roles:**\n    *   **Project Lead:** Oversees the entire process, makes final decisions.\n    *   **Search Strategist:** Designs and manages search alerts, troubleshoots database issues.\n    *   **Screeners:** Two independent screeners per article for title/abstract and full-text.\n    *   **Conflict Resolvers:** A third person to resolve screening conflicts.\n    *   **Data Extractors:** Two independent extractors per study.\n    *   **Methodologists/Statisticians:** For quality assessment, synthesis, and statistical analysis.\n    *   **Technology Manager:** Troubleshoots software, manages data flow.\n    *   *Key for LSR:* Ensure clear handover processes and training for new team members.\n\n3.  **Technology Stack Selection:**\n    *   **Reference Management:** While good for storing, dedicated SR software is essential for screening.\n    *   **Systematic Review Software (Mandatory for LSR):**\n        *   **Covidence:** User-friendly, good for dual screening, deduplication, full-text retrieval, data extraction. Integrates with PubMed.\n        *   **Rayyan QCRI:** Free, intuitive, supports dual screening, deduplication, blinding, and conflict resolution. Good for initial setup.\n        *   **DistillerSR:** Highly customizable, powerful for complex reviews, excellent for data extraction, but can be more expensive/complex.\n        *   **EPPI-Reviewer:** Comprehensive, flexible, but has a steeper learning curve.\n        *   *Choose one that supports continuous import of new records and tracks screening progress for only NEW records.*\n\n---\n\n### Phase 2: Ongoing Search Infrastructure\n\nThis is the backbone of your living review.\n\n1.  **Develop a Highly Sensitive Search Strategy:**\n    *   **Core Terms:** Combine MeSH/Emtree terms and free-text keywords for COVID-19, vaccines, effectiveness, efficacy, variants (e.g., \u0022COVID-19,\u0022 \u0022SARS-CoV-2,\u0022 \u0022vaccine,\u0022 \u0022vaccination,\u0022 \u0022effectiveness,\u0022 \u0022efficacy,\u0022 \u0022real-world,\u0022 \u0022Omicron,\u0022 \u0022Delta\u0022).\n    *   **Date Restriction:** Crucially, design the search so you can easily restrict by *last search date*. This ensures you only retrieve new articles.\n    *   **Pilot Testing:** Run the initial search and check for sensitivity (do you find known key studies?) and specificity (is the volume manageable?).\n\n2.  **Identify Core Databases \u0026 Grey Literature Sources:**\n    *   **Biomedical Databases (Essential for Alerts):**\n        *   **PubMed/MEDLINE:** Set up \u0022My NCBI\u0022 alerts.\n        *   **Embase:** Set up alerts (often via institution). Excellent for drug-related literature.\n        *   **Web of Science/Scopus:** Set up alerts. Good for broader coverage.\n    *   **Preprint Servers (CRITICAL for COVID-19):**\n        *   **medRxiv/bioRxiv:** Set up RSS feeds or use their search alert functions. Often have the newest evidence.\n        *   *Challenge:* Preprints are not peer-reviewed. You\u0027ll need a clear policy on their inclusion and how to update if a peer-reviewed version emerges.\n    *   **Specialized COVID-19 Repositories:**\n        *   **LitCovid (NIH/NLM):** Daily updates on COVID-19 literature. You can often set up alerts based on keywords.\n        *   **WHO COVID-19 Global literature on coronavirus disease:** Another excellent, curated resource.\n        *   **CDC/ECDC databases:** For surveillance reports, technical briefs.\n    *   **Clinical Trial Registries:**\n        *   **ClinicalTrials.gov, WHO ICTRP:** Set up alerts for relevant new trials or results postings.\n    *   **Government/Public Health Websites:**\n        *   **CDC, WHO, UKHSA, ECDC:** Regularly check their publications section for new reports on vaccine effectiveness. (Less automatable, may require manual checks).\n    *   **Google Scholar Alerts:** Can capture some grey literature and reports missed by other alerts, but often low specificity.\n\n3.  **Implement Automated Search Alerts:**\n    *   **Database-Specific Alerts:** Go into each database (PubMed, Embase, Web of Science, Scopus) and save your search strategy. Configure email alerts to be sent *weekly or bi-weekly* with *new results only*.\n    *   **RSS Feeds:** For preprint servers and some specialized repositories, subscribe to RSS feeds for your keywords. Use an RSS reader (e.g., Feedly) to aggregate these.\n    *   **API Integrations (Advanced):** For larger teams, consider using APIs (e.g., PubMed, Crossref) to programmatically fetch new articles. This requires coding skills but offers maximum control and automation.\n\n4.  **Centralized Search Log \u0026 Import Process:**\n    *   **Search Log:** Maintain a detailed log of every search run, including:\n        *   Database/Source\n        *   Date of search\n        *   Search string used\n        *   Date range covered (e.g., \u0022since last search on YYYY-MM-DD\u0022)\n        *   Number of results retrieved\n        *   Number of results imported into SR software\n    *   **Dedicated Folder for New Results:** Create a shared drive/cloud folder (e.g., Google Drive, SharePoint) where all new search results (e.g., .nbib, .ris files) are saved by the search strategist.\n    *   **Regular Import into SR Software:** Establish a schedule (e.g., every Monday morning) for importing these new results into your chosen SR software (Covidence, Rayyan, etc.). *Crucially, the software must support adding new records to an existing review.*\n\n5.  **Automated Deduplication:**\n    *   Your chosen SR software (Covidence, Rayyan, DistillerSR) should have robust deduplication capabilities.\n    *   Run deduplication immediately after importing new records.\n    *   Manual checks of potential duplicates may still be necessary, especially for records from diverse sources (e.g., preprints vs. published articles).\n\n---\n\n### Phase 3: Continuous Screening Workflow\n\nThis is where the team actively engages with the new literature.\n\n1.  **Clear \u0026 Granular Inclusion/Exclusion Criteria:**\n    *   **Detailed Document:** Have an accessible, detailed document outlining criteria, with examples. This is essential for consistency across screeners and over time.\n    *   **Key for LSR:** Clarify how to handle updates to previous studies (e.g., preprint now published, new results from an ongoing trial).\n\n2.  **Scheduled Screening Sprints:**\n    *   **Regular Intervals:** Based on your defined update frequency (e.g., weekly or bi-weekly), schedule dedicated \u0022screening sprints\u0022 for your team.\n    *   **Team Capacity:** Ensure your team has sufficient capacity to handle the incoming volume. COVID-19 vaccine effectiveness literature is high volume.\n    *   **Focused Work:** Encourage screeners to focus on screening during these times to maintain momentum.\n\n3.  **Screening Process within SR Software:**\n    *   **Title/Abstract Screening:**\n        *   Two independent screeners review each new title/abstract.\n        *   SR software should present only *new, undeduped* articles.\n        *   Decision: Include, Exclude, or \u0022Maybe\u0022 (for uncertainty).\n    *   **Conflict Resolution (Title/Abstract):**\n        *   The SR software highlights conflicts.\n        *   Conflicts are resolved by a third reviewer or team discussion before proceeding.\n    *   **Full-Text Retrieval:**\n        *   For all \u0022Include\u0022 and \u0022Maybe\u0022 articles, retrieve the full text. Set up automated DOI lookup/link resolvers if possible.\n        *   *Challenge:* Many COVID-19 papers are behind paywalls. Ensure institutional access or utilize interlibrary loan services quickly.\n    *   **Full-Text Screening:**\n        *   Two independent screeners review the full text against the precise inclusion/exclusion criteria.\n        *   Record specific exclusion reasons for articles rejected at this stage. This is crucial for transparency.\n    *   **Conflict Resolution (Full-Text):**\n        *   Resolve conflicts, documenting decisions.\n\n4.  **Calibration \u0026 Training:**\n    *   **Initial Calibration:** Before starting, screen a small set of articles together to ensure consistent interpretation of criteria.\n    *   **Ongoing Calibration:** Periodically (e.g., quarterly or when criteria are updated) re-calibrate with a small sample of *new* articles, especially if new team members join.\n    *   **Regular Check-ins:** Hold brief team meetings after each screening sprint to discuss challenges, clarify criteria, and maintain morale.\n\n5.  **Documentation of Screening Decisions:**\n    *   The SR software automatically logs individual and final screening decisions.\n    *   Maintain a clear log of exclusion reasons for full-text articles. This will be part of your PRISMA flow diagram for each update cycle.\n\n---\n\n### Phase 4: Supporting Infrastructure \u0026 Best Practices for LSR\n\n1.  **Version Control \u0026 Audit Trail:**\n    *   **SR Software:** Most SR software will track changes and screening decisions.\n    *   **Protocol:** Any amendments to the protocol must be dated, logged, and registered (e.g., PROSPERO amendment).\n    *   **Search Strategies:** Keep dated versions of all search strategies.\n    *   **PRISMA Flow Diagram:** You\u0027ll need a cumulative PRISMA diagram for the entire review, and potentially mini-diagrams for each update cycle showing *new* additions.\n\n2.  **Data Extraction \u0026 Quality Assessment (Briefly Mention):**\n    *   While not directly search/screening, this phase is the next step.\n    *   **Standardized Forms:** Use pre-piloted, standardized data extraction forms within your SR software (or custom forms).\n    *   **Living Data Management:** How will data be stored and updated? A relational database or highly structured spreadsheet (e.g., Google Sheets, Excel with version control) linked to your SR software is ideal.\n    *   **Risk of Bias/Quality Assessment:** Select appropriate tools (e.g., RoB 2.0 for RCTs, ROBINS-I for observational studies) and integrate into your workflow.\n\n3.  **Communication \u0026 Dissemination:**\n    *   **Dedicated Platform:** Consider a website or online platform to host the LSR, showing the latest version, previous updates, and a transparent audit trail.\n    *   **Alerts for Stakeholders:** Provide an option for stakeholders (policymakers, clinicians, public) to subscribe to email alerts for new updates.\n    *   **Publication Strategy:** Decide if you will publish cumulative review updates in journals or maintain a continuously updated living document online.\n\n4.  **Sustainability \u0026 Funding:**\n    *   LSRs are resource-intensive. Plan for long-term funding, technology subscriptions, and team compensation.\n\n---\n\n**Key Considerations for a Living Review on COVID-19 Vaccine Effectiveness:**\n\n*   **Volume:** The sheer volume of COVID-19 literature is immense. Be prepared for a high screening load. Prioritize core outcomes if volume becomes unmanageable.\n*   **Speed:** Evidence changes rapidly. Aim for swift processing from search to synthesis.\n*   **Preprints:** Embrace preprints but have clear rules for their inclusion, critical appraisal, and how to handle peer-reviewed versions.\n*   **Heterogeneity:** Vaccine effectiveness varies by vaccine type, dose, population, variant, and outcome. Your extraction and synthesis need to account for this complexity.\n*   **Duplicate Reporting:** Many studies come from the same datasets or groups. Be vigilant about identifying overlapping populations or analyses.\n\nBy meticulously setting up this infrastructure, you\u0027ll build a robust and sustainable living systematic review capable of providing timely, evidence-based insights into COVID-19 vaccine effectiveness. Good luck!", "Grok": "### 1. Setting Up Infrastructure for Ongoing Searches and Screening\n\nA living systematic review (LSR) on COVID-19 vaccine effectiveness requires scalable, automated, and collaborative tools to handle continuous data influx. The goal is to efficiently search for new evidence, screen it, and integrate updates without manual overload. Below, I\u0027ll outline a step-by-step setup, drawing from established methodologies like those from Cochrane or the Joanna Briggs Institute for LSRs. This focuses on practicality for a team (e.g., researchers, librarians).\n\n#### Step 1: Define Search Scope and Parameters\n- **Refine your PICO (Population, Intervention, Comparator, Outcome)**: For COVID-19 vaccines, specify e.g., Population: Adults/children; Intervention: mRNA, viral vector, etc.; Comparator: Placebo/unvaccinated; Outcome: Effectiveness against infection, hospitalization, variants (e.g., Omicron subvariants).\n- **Establish update frequency**: Aim for weekly or bi-weekly searches initially, scaling to monthly as literature stabilizes. Document protocols in a living review plan (e.g., using PRISMA-L guidelines extended for living reviews).\n- **Inclusion/exclusion criteria**: Pre-define (e.g., RCTs, cohort studies on VE \u226550%; exclude preprints unless peer-reviewed). Use tools like Covidence or Rayyan for shared criteria.\n\n#### Step 2: Automate Literature Searches\nUse databases and alerting systems to monitor new publications. Avoid manual PubMed checks\u2014opt for automation.\n\n- **Core Databases and Sources**:\n  - PubMed/MEDLINE (free, comprehensive for biomedical).\n  - Embase (for international coverage; subscription needed).\n  - Web of Science or Scopus (for broader indexing).\n  - COVID-specific: WHO COVID-19 Research Database, medRxiv/bioRxiv for preprints (but verify before inclusion).\n  - Grey literature: ClinicalTrials.gov, WHO ICTRP for trials; Google Scholar alerts for policy docs.\n\n- **Search Automation Tools**:\n  - **PubMed Alerts**: Set up saved searches (e.g., \u0022COVID-19 vaccine effectiveness\u0022 AND (RCT OR cohort) with date filters). Email alerts deliver new hits. Use MeSH terms like \u0022COVID-19 Vaccines/therapeutic use\u0022 for precision.\n  - **EndNote Click or Zotero with RSS/Feeds**: Integrate with databases for automated imports. Zotero\u0027s web translator pulls PDFs automatically.\n  - **Advanced Automation**:\n    - **LitSuggest or ASReview**: AI-driven tools for deduplication and relevance ranking. ASReview uses active learning to prioritize screening (free/open-source).\n    - **Europe PMC or Semantic Scholar APIs**: Script weekly pulls using Python (e.g., via \u0060biopython\u0060 library for PubMed) into a shared repository.\n    - **Commercial Options**: Covidence integrates searches from multiple sources with auto-deduplication (subscription ~$240/year for teams).\n  - **Tip**: Export searches to RIS/EndNote XML format for easy import. Run pilot searches to test ~500-1000 hits/month volume.\n\n- **Handling Updates**: Tag new results by date (e.g., \u0022post-2023\u0022). Use version control (e.g., GitHub) for search strings to track changes.\n\n#### Step 3: Implement Screening Workflow\nScreening must be efficient, bias-minimized, and team-based to handle volume (expect 100-500 new records per update cycle).\n\n- **Collaboration Platforms**:\n  - **Covidence (Recommended)**: Cloud-based; automates title/abstract screening, full-text retrieval (via Unpaywall), risk-of-bias assessment (e.g., RoB 2 for RCTs). Supports blind duplicate screening by 2\u002B reviewers. Exports to RevMan for meta-analysis. Free for small teams up to 2 reviews.\n  - **Rayyan (Free Alternative)**: Web/app-based; AI prioritization (e.g., highlights similar abstracts). Great for crowdsourcing screening. Integrates with PubMed exports.\n  - **DistillerSR**: Enterprise-level for large teams; tracks decisions and conflicts resolution.\n\n- **Screening Process**:\n  1. **Level 1 (Title/Abstract)**: Two independent reviewers use the tool\u0027s blinding feature. Resolve conflicts via discussion or third reviewer (use tool\u0027s voting system).\n  2. **Level 2 (Full-Text)**: Retrieve PDFs automatically (integrate with browser extensions like Unpaywall or Kopernio for paywalls). Assess eligibility against criteria.\n  3. **Data Extraction**: Use standardized forms in Excel/Google Sheets or integrated tools (e.g., Covidence templates for VE metrics like risk ratios, confidence intervals by variant/age group).\n  - **Efficiency Boosts**: Train on 10% sample for calibration. Use machine learning in ASReview to reduce screening by 50-70% (it learns from your decisions).\n  - **Audit Trail**: All tools log timestamps, decisions, and reasons for exclusion\u2014essential for transparency.\n\n- **Team Management**: Assign roles (e.g., searcher, screener1, screener2) via shared dashboards. Use Slack/Teams integrations for notifications on new batches.\n\n#### Step 4: Data Management and Integration\n- **Repository**: Store everything in a central, versioned system.\n  - **GitHub or OSF (Open Science Framework)**: For protocols, search logs, and datasets. Free and collaborative.\n  - **Google Drive/Dropbox**: For raw exports and PDFs; ensure access controls.\n- **Deduplication**: Built into Covidence/Rayyan; otherwise, use EndNote\u0027s duplicate finder or Python scripts (e.g., with \u0060pandas\u0060).\n- **Scalability Considerations**: Start small (manual for first cycle), then automate. Budget for tools (~$500-2000/year for a team). Monitor workload\u2014 if \u003E20 hours/week, hire a librarian or use outsourcing (e.g., via Evidence Prime).\n- **Ethical/Compliance**: Register on PROSPERO (adapt for living reviews). Comply with data privacy (e.g., GDPR for international teams) and report funding conflicts.\n\nThis setup should launch your LSR infrastructure in 1-2 weeks with testing. Once running, it feeds directly into synthesis (e.g., updating meta-analyses in RevMan or R with \u0060metafor\u0060 package). If you need guidance on synthesis, risk-of-bias, or dissemination (e.g., via a living website), let me know for the next steps!"}' published-date="{DateTime.UtcNow:HH:mm}"
/>

## Analysis

This comparison demonstrates the different approaches each AI model takes when responding to the same prompt. Here are the key differences observed:

### Response Characteristics

**Claude**: Provides a detailed response with 15 sentences. 

**Gemini**: Provides a direct response with 175 sentences. 

**Grok**: Provides a direct response with 103 sentences. 

## Key Takeaways

- Each model brings unique strengths to this type of query
- Response styles vary significantly between models
- Consider your specific use case when choosing between these models

## Try This Comparison Yourself

Want to test these models with your own prompts? Visit [SNEOS.com](https://sneos.com) to compare AI responses side-by-side in real-time.

---

*This comparison was generated using the [SNEOS AI Comparison Tool](https://sneos.com)*
*Published: October 15, 2025 | Models: Claude, Gemini, Grok*
