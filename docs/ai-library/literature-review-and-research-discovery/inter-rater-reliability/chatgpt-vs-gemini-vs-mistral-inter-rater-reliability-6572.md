---
title: "I'm conducting dual screening for a systematic review with a colleague. Help me: 1) Calculate Cohen's kappa or other agreement statistics after screening a pilot set"
description: "AI model comparison: I'm conducting dual screening for a systematic review with a colleague. Help me: 1) Calculate Cohen'..."
date: 2025-10-15
category: comparison
layout: Clean
tags:
  - ChatGPT
  - Gemini
  - Mistral
  - AI Comparison
author: SNEOS Community
head:
  - - meta
    - name: description
      content: "AI model comparison: I'm conducting dual screening for a systematic review with a colleague. Help me: 1) Calculate Cohen's kappa or other agr..."
---

<ChatComparison
  title="I'm conducting dual screening for a systematic review with a colleague. Help me: 1) Calculate Cohen's kappa or other agreement statistics after screening a pilot set"
  prompt="I'm conducting dual screening for a systematic review with a colleague. Help me: 1) Calculate Cohen's kappa or other agreement statistics after screening a pilot set"
  :responses='{"ChatGPT": "Certainly! When conducting a systematic review and performing dual screening, it\u0027s important to measure the level of agreement between the reviewers. One commonly used statistic for this purpose is Cohen\u0027s kappa. Here\u0027s how to calculate it, as well as some additional agreement statistics you might consider.\n\n### Steps to Calculate Cohen\u0027s Kappa\n\n1. **Conduct the Screening**: Each reviewer independently screens a set of studies. For example, let\u0027s say you both screened 50 studies.\n\n2. **Create a Contingency Table**: After screening, you will categorize studies as either \u0022included\u0022 or \u0022excluded\u0022. A 2x2 contingency table would look like this:\n\n   |               | Reviewer 2: Included | Reviewer 2: Excluded |\n   |---------------|---------------------|-----------------------|\n   | **Reviewer 1: Included**   | a (both included)    | b (Reviewer 1 included, Reviewer 2 excluded)   |\n   | **Reviewer 1: Excluded**   | c (Reviewer 1 excluded, Reviewer 2 included) | d (both excluded)   |\n\n   Where:\n   - \u0060a\u0060 = Number of studies both included\n   - \u0060b\u0060 = Number of studies where only Reviewer 1 included\n   - \u0060c\u0060 = Number of studies where only Reviewer 2 included\n   - \u0060d\u0060 = Number of studies both excluded\n\n3. **Calculate Observed Agreement (Po)**:\n   \\[\n   P_o = \\frac{a \u002B d}{N}\n   \\]\n   where \\( N \\) is the total number of studies screened (\\(N = a \u002B b \u002B c \u002B d\\)).\n\n4. **Calculate Expected Agreement (Pe)**:\n   \\[\n   P_e = \\left(\\frac{(a\u002Bb)(a\u002Bc)}{N^2}\\right) \u002B \\left(\\frac{(c\u002Bd)(b\u002Bd)}{N^2}\\right)\n   \\]\n   This is computed by taking the proportion of included studies by each reviewer and squaring it, as well as doing the same for excluded studies.\n\n5. **Calculate Cohen\u0027s Kappa (\u03BA)**:\n   \\[\n   \\kappa = \\frac{P_o - P_e}{1 - P_e}\n   \\]\n\n### Example Calculation\nSuppose after screening you have the following results:\n- a = 30\n- b = 5\n- c = 8\n- d = 7\n\nCalculating:\n- Total \\( N = a \u002B b \u002B c \u002B d = 30 \u002B 5 \u002B 8 \u002B 7 = 50 \\)\n- Observed Agreement \\( P_o = \\frac{30 \u002B 7}{50} = \\frac{37}{50} = 0.74 \\)\n- Expected Agreement Calculation:\n  - \\( \\frac{(30 \u002B 5)(30 \u002B 8)}{50^2} = \\frac{35 \\cdot 38}{2500} = \\frac{1330}{2500} = 0.532 \\)\n  - \\( \\frac{(8 \u002B 7)(5 \u002B 7)}{50^2} = \\frac{15 \\cdot 12}{2500} = \\frac{180}{2500} = 0.072 \\)\n  - Combine these: \\( P_e = 0.532 \u002B 0.072 = 0.604 \\)\n\nNow calculate Kappa:\n\\[\n\\kappa = \\frac{0.74 - 0.604}{1 - 0.604} = \\frac{0.136}{0.396} \\approx 0.343\n\\]\n\n### Additional Agreement Statistics\n1. **Percentage Agreement**: A simpler measure that states the percentage of times the reviewers agree. This is:\n   \\[\n   \\text{Percentage Agreement} = \\frac{a \u002B d}{N} \\times 100\n   \\]\n\n2. **Fleiss\u0027 Kappa**: If there are more than two reviewers or if you want to measure agreement for multiple reviewers.\n\n3. **Krippendorff\u0027s Alpha**: A versatile measure of inter-rater reliability that can be used for different levels of measurement.\n\n4. **Scott\u2019s Pi**: Another chance-corrected measure of agreement.\n\nUsing Cohen\u0027s Kappa is a widely accepted choice for two independent raters. Consider calculating these additional metrics depending on your needs and data structure. If you have further specifics, feel free to ask!", "Gemini": "This is a crucial step in a systematic review! Calculating agreement statistics after a pilot screening helps ensure your inclusion/exclusion criteria are clear and consistently applied before you screen the full set of articles.\n\nLet\u0027s break down how to calculate Cohen\u0027s Kappa and other relevant statistics, along with what to do with the results.\n\n---\n\n## 1. Calculating Cohen\u0027s Kappa (or other agreement statistics)\n\nCohen\u0027s Kappa ($\\kappa$) is the most widely used agreement statistic for systematic reviews because it accounts for the agreement that would be expected by chance.\n\n### Data Setup\n\nFirst, you need to record the decisions of both screeners for each article in your pilot set. A simple spreadsheet works best.\n\n| Article ID | Reviewer 1 Decision | Reviewer 2 Decision |\n| :--------- | :------------------ | :------------------ |\n| Article 1  | Include             | Include             |\n| Article 2  | Exclude             | Exclude             |\n| Article 3  | Include             | Exclude             |\n| Article 4  | Exclude             | Include             |\n| Article 5  | Exclude             | Exclude             |\n| ...        | ...                 | ...                 |\n\nYou\u0027ll categorize each decision as either \u0022Include\u0022 (I) or \u0022Exclude\u0022 (E).\n\n### Step-by-Step Calculation for Cohen\u0027s Kappa\n\n#### Step 1: Create a 2x2 Contingency Table\n\nCount the number of articles that fall into each of these four categories:\n\n|                | **Reviewer 2: Include** | **Reviewer 2: Exclude** | **Total (Reviewer 1)** |\n| :------------- | :---------------------- | :---------------------- | :--------------------- |\n| **Reviewer 1: Include** | A (Agreed Include)      | B (R1 Incl, R2 Excl)    | A \u002B B                  |\n| **Reviewer 1: Exclude** | C (R1 Excl, R2 Incl)    | D (Agreed Exclude)      | C \u002B D                  |\n| **Total (Reviewer 2)** | A \u002B C                   | B \u002B D                   | N (Total Articles)     |\n\n*   **A:** Both reviewers included the article.\n*   **B:** Reviewer 1 included, Reviewer 2 excluded.\n*   **C:** Reviewer 1 excluded, Reviewer 2 included.\n*   **D:** Both reviewers excluded the article.\n*   **N:** Total number of articles in your pilot.\n\n#### Step 2: Calculate Observed Agreement ($P_o$)\n\nThis is the proportion of articles where both reviewers agreed, regardless of whether they agreed to include or exclude.\n\n$P_o = (A \u002B D) / N$\n\n#### Step 3: Calculate Expected Agreement by Chance ($P_e$)\n\nThis is where Kappa differs from simple percentage agreement. It accounts for how much agreement you\u0027d expect just by random chance.\n\n$P_e = [((A\u002BB)/N) * ((A\u002BC)/N)] \u002B [((C\u002BD)/N) * ((B\u002BD)/N)]$\n\nLet\u0027s break down $P_e$:\n*   \u0060((A\u002BB)/N)\u0060: Proportion of articles Reviewer 1 *included*.\n*   \u0060((A\u002BC)/N)\u0060: Proportion of articles Reviewer 2 *included*.\n*   \u0060((C\u002BD)/N)\u0060: Proportion of articles Reviewer 1 *excluded*.\n*   \u0060((B\u002BD)/N)\u0060: Proportion of articles Reviewer 2 *excluded*.\n\nThe first part of the sum \u0060[((A\u002BB)/N) * ((A\u002BC)/N)]\u0060 is the probability that both reviewers *randomly* include an article.\nThe second part of the sum \u0060[((C\u002BD)/N) * ((B\u002BD)/N)]\u0060 is the probability that both reviewers *randomly* exclude an article.\n\n#### Step 4: Calculate Cohen\u0027s Kappa ($\\kappa$)\n\nNow, plug $P_o$ and $P_e$ into the Kappa formula:\n\n$\\kappa = (P_o - P_e) / (1 - P_e)$\n\n---\n\n### Example Calculation\n\nLet\u0027s say your pilot set has 100 articles:\n\n*   **A (Agreed Include):** 15 articles\n*   **B (R1 Incl, R2 Excl):** 5 articles\n*   **C (R1 Excl, R2 Incl):** 10 articles\n*   **D (Agreed Exclude):** 70 articles\n*   **N (Total):** 15 \u002B 5 \u002B 10 \u002B 70 = 100 articles\n\n**1. 2x2 Table:**\n\n|                | **R2: Include** | **R2: Exclude** | **Total (R1)** |\n| :------------- | :-------------- | :-------------- | :------------- |\n| **R1: Include** | 15              | 5               | 20             |\n| **R1: Exclude** | 10              | 70              | 80             |\n| **Total (R2)** | 25              | 75              | **100**        |\n\n**2. Observed Agreement ($P_o$):**\n$P_o = (15 \u002B 70) / 100 = 85 / 100 = 0.85$\n\n**3. Expected Agreement by Chance ($P_e$):**\n*   Reviewer 1 Included: $(15\u002B5)/100 = 20/100 = 0.20$\n*   Reviewer 2 Included: $(15\u002B10)/100 = 25/100 = 0.25$\n*   Reviewer 1 Excluded: $(10\u002B70)/100 = 80/100 = 0.80$\n*   Reviewer 2 Excluded: $(5\u002B70)/100 = 75/100 = 0.75$\n\n$P_e = (0.20 * 0.25) \u002B (0.80 * 0.75)$\n$P_e = 0.05 \u002B 0.60$\n$P_e = 0.65$\n\n**4. Cohen\u0027s Kappa ($\\kappa$):**\n$\\kappa = (0.85 - 0.65) / (1 - 0.65)$\n$\\kappa = 0.20 / 0.35$\n$\\kappa \\approx 0.57$\n\n---\n\n### Interpretation of Cohen\u0027s Kappa\n\nKappa values range from -1 to 1:\n*   **1:** Perfect agreement.\n*   **0:** Agreement is no better than chance.\n*   **\u003C 0:** Agreement is worse than chance (very rare in dual screening).\n\nCommonly cited (though somewhat arbitrary) guidelines for interpreting Kappa values (Landis \u0026 Koch, 1977):\n*   **\u003C 0.00:** Poor agreement\n*   **0.00 \u2013 0.20:** Slight agreement\n*   **0.21 \u2013 0.40:** Fair agreement\n*   **0.41 \u2013 0.60:** Moderate agreement\n*   **0.61 \u2013 0.80:** Substantial agreement\n*   **0.81 \u2013 1.00:** Almost perfect agreement\n\nFor systematic reviews, a Kappa of **0.60 or higher** is often considered acceptable for a pilot screening, indicating substantial agreement. Some protocols may aim for 0.70 or 0.80.\n\n---\n\n### Other Agreement Statistics (and why Kappa is often preferred)\n\nWhile Kappa is standard, you might also consider:\n\n1.  **Percentage Agreement (Observed Agreement $P_o$):**\n    *   **Calculation:** $(A \u002B D) / N$ (as calculated for Kappa).\n    *   **Pros:** Very easy to understand and calculate.\n    *   **Cons:** It doesn\u0027t account for agreement that would happen purely by chance. If you have a very high prevalence of \u0022Exclude\u0022 decisions, even random guessing might lead to high observed agreement for \u0022Exclude,\u0022 inflating this percentage. This is why Kappa is generally preferred.\n\n2.  **Gwet\u0027s AC1/AC2:**\n    *   **Description:** Gwet\u0027s AC1 (for nominal data) is an alternative to Kappa that can be more robust to what\u0027s called the \u0022prevalence paradox\u0022 and \u0022marginal-probability paradox.\u0022 These paradoxes can cause Kappa to be low even when observed agreement is high, especially if one category is very rare or very common.\n    *   **Pros:** Can provide a more stable measure of agreement in certain scenarios.\n    *   **Cons:** Less commonly reported than Kappa in systematic reviews, so your audience might be less familiar with it. Calculation is more complex and usually requires software.\n\n**Recommendation:** Start with **Cohen\u0027s Kappa**. It\u0027s the standard for good reason. If you encounter a situation where you have very high observed agreement but a surprisingly low Kappa, Gwet\u0027s AC1 might be worth exploring, but for most systematic reviews, Kappa is sufficient.\n\n---\n\n### Tools for Calculation\n\n*   **Excel/Google Sheets:** You can set up the 2x2 table and input the formulas directly.\n    *   For the example above:\n        *   \u0060A\u0060 in cell B2, \u0060B\u0060 in C2, \u0060C\u0060 in B3, \u0060D\u0060 in C3.\n        *   \u0060N\u0060 (total articles) in D4: \u0060=SUM(B2:C3)\u0060\n        *   \u0060Po\u0060: \u0060=(B2\u002BC3)/D4\u0060\n        *   \u0060Pe\u0060: \u0060=( ( (B2\u002BC2)/D4 ) * ( (B2\u002BB3)/D4 ) ) \u002B ( ( (C3\u002BB3)/D4 ) * ( (C2\u002BC3)/D4 ) )\u0060 (Make sure parentheses are correct!)\n        *   \u0060Kappa\u0060: \u0060=(Po-Pe)/(1-Pe)\u0060\n*   **Online Calculators:**\n    *   Search for \u0022Cohen\u0027s Kappa calculator online.\u0022 Many free options are available where you just input your A, B, C, D values.\n*   **Statistical Software (R, Python, SPSS, Stata):**\n    *   These packages have functions for calculating Kappa and often provide confidence intervals, which are useful.\n    *   **R:** \u0060vcd\u0060 package (\u0060Kappa()\u0060 function), \u0060psych\u0060 package (\u0060cohen.kappa()\u0060 function).\n    *   **Python:** \u0060scikit-learn\u0060 library (\u0060cohen_kappa_score()\u0060 function).\n\n---\n\n## 2. What to do with the results of your pilot screening\n\nCalculating Kappa is just the first step. The real value of a pilot is in improving your process.\n\n### If Kappa is Acceptable (e.g., \u003E 0.60):\n\n1.  **Briefly Review Discrepancies:** Even with good Kappa, review the few articles where you disagreed (cells B and C in your 2x2 table).\n2.  **Confirm Clarity:** Are the criteria robust enough for the full screening?\n3.  **Proceed to Full Screening:** You can now screen the full set of articles with reasonable confidence in your consistency.\n\n### If Kappa is Low (e.g., \u003C 0.60): This is a learning opportunity!\n\nA low Kappa indicates that your reviewers are not applying the criteria consistently enough. This is precisely *why* you do a pilot.\n\n1.  **Thorough Discrepancy Analysis:**\n    *   **Discuss ALL disagreements (cells B and C).** Go through each article where you disagreed.\n    *   **Identify the ROOT CAUSE of disagreement:**\n        *   **Ambiguous Criteria:** Are your inclusion/exclusion criteria unclear, subjective, or too broad/narrow in specific situations? (e.g., \u0022Relevant population\u0022 \u2013 what defines \u0022relevant\u0022?)\n        *   **Lack of Specificity:** Do your criteria need more explicit definitions or examples?\n        *   **Misinterpretation:** Did one reviewer simply misunderstand a criterion?\n        *   **Data Extraction Error:** Did one reviewer miss a key piece of information in the abstract/title?\n        *   **Inconsistent Application:** Even with clear criteria, were they applied differently?\n    *   **Consensus:** For each disagreed article, reach a consensus decision and document *why* that decision was made based on your criteria.\n\n2.  **Refine Your Screening Protocol:**\n    *   **Clarify Criteria:** Rewrite problematic criteria to be more explicit, objective, and unambiguous. Add bullet points, keywords to look for/avoid, and specific examples.\n    *   **Update Screening Tool:** If you\u0027re using a screening form, update it to reflect the refined criteria. Add notes or pop-up text to guide reviewers on common ambiguities.\n    *   **Develop FAQs/Decision Rules:** Create a list of common \u0022edge cases\u0022 from your pilot and how they should be handled.\n    *   **Review Definitions:** Ensure everyone understands key terms.\n\n3.  **Conduct Another Pilot (Repeat):**\n    *   Take a *new* set of articles (or a subset of your full pool that hasn\u0027t been screened yet) or re-screen the original pilot set *after* refining the criteria.\n    *   Calculate Kappa again.\n    *   Repeat this iterative process until you achieve an acceptable Kappa.\n\n**Important Note:** The goal of the pilot is *not* just to get a good Kappa score, but to ensure that the criteria are clear enough for independent, consistent application. Don\u0027t be discouraged by a low Kappa initially; it just means you\u0027ve identified areas for improvement.\n\nGood luck with your systematic review!", "Mistral": "Certainly! Calculating Cohen\u0027s kappa is a common method to assess inter-rater agreement in dual screening for systematic reviews. Here\u2019s a step-by-step guide to help you calculate Cohen\u0027s kappa:\n\n### Step 1: Create a Contingency Table\nFirst, you need to create a contingency table that shows the agreement and disagreement between you and your colleague. The table will look something like this:\n\n|                | Colleague Includes | Colleague Excludes | Total |\n|----------------|--------------------|--------------------|-------|\n| You Include    | a                  | b                  | a\u002Bb   |\n| You Exclude    | c                  | d                  | c\u002Bd   |\n| Total          | a\u002Bc                | b\u002Bd                | N     |\n\n- **a**: Number of articles both you and your colleague include.\n- **b**: Number of articles you include but your colleague excludes.\n- **c**: Number of articles you exclude but your colleague includes.\n- **d**: Number of articles both you and your colleague exclude.\n- **N**: Total number of articles screened.\n\n### Step 2: Calculate Observed Agreement (P_o)\nObserved agreement is the proportion of times the raters agree.\n\n\\[ P_o = \\frac{a \u002B d}{N} \\]\n\n### Step 3: Calculate Expected Agreement (P_e)\nExpected agreement is the proportion of times the raters would agree by chance.\n\n\\[ P_e = \\frac{(a\u002Bb)(a\u002Bc) \u002B (c\u002Bd)(b\u002Bd)}{N^2} \\]\n\n### Step 4: Calculate Cohen\u0027s Kappa\nCohen\u0027s kappa is calculated as:\n\n\\[ \\kappa = \\frac{P_o - P_e}{1 - P_e} \\]\n\n### Example Calculation\nLet\u0027s say you and your colleague screened 100 articles and your contingency table looks like this:\n\n|                | Colleague Includes | Colleague Excludes | Total |\n|----------------|--------------------|--------------------|-------|\n| You Include    | 40                | 10                 | 50    |\n| You Exclude    | 5                 | 45                 | 50    |\n| Total          | 45                | 55                 | 100   |\n\n1. **Observed Agreement (P_o)**:\n\\[ P_o = \\frac{40 \u002B 45}{100} = \\frac{85}{100} = 0.85 \\]\n\n2. **Expected Agreement (P_e)**:\n\\[ P_e = \\frac{(40\u002B10)(40\u002B5) \u002B (5\u002B45)(10\u002B45)}{100^2} \\]\n\\[ P_e = \\frac{(50)(45) \u002B (50)(55)}{10000} \\]\n\\[ P_e = \\frac{2250 \u002B 2750}{10000} \\]\n\\[ P_e = \\frac{5000}{10000} = 0.50 \\]\n\n3. **Cohen\u0027s Kappa**:\n\\[ \\kappa = \\frac{0.85 - 0.50}{1 - 0.50} \\]\n\\[ \\kappa = \\frac{0.35}{0.50} \\]\n\\[ \\kappa = 0.70 \\]\n\n### Interpretation\nCohen\u0027s kappa values are typically interpreted as follows:\n- \u2264 0: No agreement\n- 0.01 \u2013 0.20: Slight agreement\n- 0.21 \u2013 0.40: Fair agreement\n- 0.41 \u2013 0.60: Moderate agreement\n- 0.61 \u2013 0.80: Substantial agreement\n- 0.81 \u2013 1.00: Almost perfect agreement\n\nIn this example, a kappa of 0.70 indicates substantial agreement between you and your colleague.\n\n### Other Agreement Statistics\nIf you are interested in other agreement statistics, you might consider:\n- **Percent Agreement**: Simply the proportion of agreements (P_o).\n- **Fleiss\u0027 Kappa**: Useful for more than two raters.\n- **Krippendorff\u0027s Alpha**: Useful for nominal, ordinal, interval, or ratio data.\n\nFor most dual screening tasks in systematic reviews, Cohen\u0027s kappa is sufficient and widely accepted."}' published-date="{DateTime.UtcNow:HH:mm}"
/>

## Analysis

This comparison demonstrates the different approaches each AI model takes when responding to the same prompt. Here are the key differences observed:

### Response Characteristics

**ChatGPT**: Provides a direct response with 37 sentences. 

**Gemini**: Provides a direct response with 140 sentences. 

**Mistral**: Provides a direct response with 37 sentences. 

## Key Takeaways

- Each model brings unique strengths to this type of query
- Response styles vary significantly between models
- Consider your specific use case when choosing between these models

## Try This Comparison Yourself

Want to test these models with your own prompts? Visit [SNEOS.com](https://sneos.com) to compare AI responses side-by-side in real-time.

---

*This comparison was generated using the [SNEOS AI Comparison Tool](https://sneos.com)*
*Published: October 15, 2025 | Models: ChatGPT, Gemini, Mistral*
