# Complete Guide: Generating 500+ Research Queries

## 📊 Current Status

**What You Have:**
- ✅ Complete researchers section structure (components, pages, methodology)
- ✅ 54 base queries in CSV format
- ✅ Template comparisons ready to replicate
- ✅ All infrastructure to support 500+ comparisons

**What You Need:**
- 🎯 450+ additional queries (to reach 500+ total)
- ⏱️ Efficient generation method
- 🔍 Quality control process

---

## 🚀 Fastest Method: AI-Assisted Generation

### Why AI Generation?

**Manual Creation:**
- Time: ~30 min per query
- 450 queries × 30 min = **225 hours (5.6 weeks full-time)**
- Fatigue → declining quality
- Repetitive and exhausting

**AI-Assisted Creation:**
- Time: ~15 min per batch of 50 queries
- 10 batches × 15 min generation + 10 min review = **4-5 hours total**
- Consistent quality
- Easy to iterate and refine

**Efficiency Gain: 98% time savings** (225 hours → 5 hours)

---

## 📝 Step-by-Step Generation Process

### Step 1: Prepare the Prompt Template

Use this master prompt (copy-paste ready):

```
I'm building a comprehensive library of AI comparison queries specifically for academic researchers. These queries will be used to generate side-by-side comparisons of ChatGPT, Claude, and Gemini for research tasks.

CONTEXT:
I already have 54 base queries covering: literature reviews, data analysis, academic writing, research methods, grant writing, ethics, and career development.

TASK:
Generate 50 additional detailed research queries for the category: **[CATEGORY NAME]**

FORMAT:
Output as CSV with these exact columns (include header row):
category,subcategory,query,target_audience,priority,use_academic_wrapper,notes

REQUIREMENTS FOR EACH QUERY:
1. Query length: 150-300 words
2. Include 5-8 numbered steps or specific tasks
3. Be highly specific and actionable
4. Target real research workflows researchers face
5. Mention specific tools, software, or methodologies when relevant
6. Vary difficulty: some for beginners, some advanced
7. Cover different academic disciplines within the category
8. Write as if researcher is asking AI for help

PRIORITY LEVELS:
- HIGH: Core researcher needs, frequently searched
- MEDIUM: Important but more specialized
- LOW: Advanced or niche topics

TARGET AUDIENCES (vary these):
- Graduate Students, PhD Students, PhD Candidates
- Postdocs, Early Career Researchers, Researchers
- Principal Investigators, Senior Faculty
- Discipline-specific (Psychology Researchers, Medical Researchers, etc.)

EXAMPLE QUERY FORMAT:
"I'm conducting a systematic literature review on X topic. Help me: 1) Develop comprehensive search strategy for PubMed/PsycINFO, 2) Create inclusion/exclusion criteria, 3) Design data extraction form, 4) Suggest quality assessment tools (PRISMA, Cochrane Risk of Bias), 5) Plan synthesis approach"

NOW GENERATE 50 QUERIES FOR: **[INSERT CATEGORY]**
```

### Step 2: Generate in Batches

Run the prompt **10 times** on ChatGPT or Claude, once for each category:

#### Batch 1: Literature Review & Research Discovery (50 queries)
**Replace `[INSERT CATEGORY]` with:**
```
Literature Review & Research Discovery

Cover these subtopics:
- Systematic reviews (PRISMA, Cochrane methods)
- Scoping reviews, rapid reviews, realist reviews
- Meta-analysis, meta-synthesis
- Citation management tools (Zotero, Mendeley, EndNote)
- Database searching (PubMed, Web of Science, Scopus, Google Scholar)
- Screening tools (Rayyan, Covidence, ASReview)
- Literature synthesis and gap analysis
- Discipline-specific variations (psychology, medicine, education, sociology, etc.)
```

#### Batch 2: Data Analysis (50 queries)
```
Quantitative Data Analysis

Cover these subtopics:
- Basic inferential stats (t-tests, ANOVA, chi-square)
- Regression (linear, logistic, hierarchical, Poisson, negative binomial)
- Advanced methods (SEM, MLM, survival analysis, propensity scores)
- Software tutorials (R/RStudio, Python/Jupyter, SPSS, STATA, SAS)
- Data preparation and cleaning
- Assumption checking and diagnostics
- Effect sizes and power analysis
- Reporting results in APA format
```

#### Batch 3: Qualitative Data Analysis (50 queries)
```
Qualitative Data Analysis

Cover these subtopics:
- Thematic analysis (Braun & Clarke method)
- Content analysis (quantitative and qualitative)
- Grounded theory, phenomenology, narrative analysis
- Discourse analysis, conversation analysis
- Software (NVivo, MAXQDA, Atlas.ti, Dedoose)
- Coding strategies and inter-rater reliability
- Writing up qualitative results
- Ensuring rigor and trustworthiness
```

#### Batch 4: Mixed Methods Research (40 queries)
```
Mixed Methods Research

Cover these subtopics:
- Integration strategies (convergent, explanatory, exploratory)
- Joint displays and meta-inferences
- Sampling and data collection planning
- Analysis and synthesis approaches
- Writing mixed methods results
- Quality criteria
- Research design variations
```

#### Batch 5: Academic Writing (60 queries)
```
Academic Writing & Publishing

Cover these subtopics:
- Manuscript structure (IMRAD)
- Specific sections (intro, methods, results, discussion)
- Different genres (empirical, theoretical, methodological, reviews)
- Revision and peer review response
- Journal selection and submission
- Abstracts and titles
- Academic writing style and clarity
- Avoiding plagiarism and ensuring integrity
```

#### Batch 6: Grant Writing (40 queries)
```
Grant Writing & Funding

Cover these subtopics:
- NIH grants (R01, R21, K awards, F awards)
- NSF proposals (different directorates)
- Foundation grants
- Specific Aims pages
- Significance and Innovation sections
- Research approach and methods
- Preliminary data
- Budget and budget justification
- Different career stages (predoctoral, postdoctoral, early career, established)
```

#### Batch 7: Research Methods & Design (50 queries)
```
Research Methods & Study Design

Cover these subtopics:
- Experimental design (RCTs, quasi-experimental, factorial)
- Survey research (design, sampling, measurement)
- Qualitative methods (interviews, ethnography, case studies, action research)
- Observational studies (cohort, case-control, cross-sectional)
- Sampling strategies
- Measurement and validity
- IRB protocols and ethics
```

#### Batch 8: Career Development (50 queries)
```
Academic Career Development

Cover these subtopics:
- Academic job market (applications, interviews, negotiations)
- Postdoc search and success
- Tenure track navigation
- Publication strategies
- Teaching and mentoring
- Service and leadership
- Work-life balance
- Alternative career paths (industry, government, nonprofit)
- Networking and collaboration
```

#### Batch 9: Research Tools & Software (40 queries)
```
Research Tools & Software

Cover these subtopics:
- Reference managers (Zotero, Mendeley, EndNote comparison)
- Statistical software (R vs Python vs SPSS vs STATA)
- Qualitative software (NVivo vs MAXQDA vs Atlas.ti)
- Writing tools (LaTeX, Overleaf, Scrivener, Word)
- Collaboration tools (OSF, GitHub, protocols.io)
- Data visualization tools
- Survey platforms (Qualtrics, SurveyMonkey, Google Forms)
- Productivity and project management
```

#### Batch 10: Open Science & Dissemination (40 queries)
```
Open Science & Research Dissemination

Cover these subtopics:
- Preregistration and registered reports
- Data sharing and FAIR principles
- Open access publishing strategies
- Preprint servers (arXiv, bioRxiv, PsyArXiv)
- Reproducibility and replication
- Conference presentations and posters
- Science communication for public
- Social media for researchers (Twitter/X, LinkedIn, ResearchGate)
```

### Step 3: Collect and Combine

**For Each Batch:**
1. Copy AI-generated CSV output
2. Paste into Excel or Google Sheets
3. Quick review for quality
4. Save as separate CSV file

**Then Combine:**
```bash
# Combine all CSVs (keeping only one header)
cd /Users/victora/Projects/SneosAutomation/

# Start with base file
cp ai-comparison-queries-researchers-section.csv researchers-queries-500plus.csv

# Append each batch (skip header row)
tail -n +2 batch1-literature-review.csv >> researchers-queries-500plus.csv
tail -n +2 batch2-quant-analysis.csv >> researchers-queries-500plus.csv
tail -n +2 batch3-qual-analysis.csv >> researchers-queries-500plus.csv
tail -n +2 batch4-mixed-methods.csv >> researchers-queries-500plus.csv
tail -n +2 batch5-writing.csv >> researchers-queries-500plus.csv
tail -n +2 batch6-grants.csv >> researchers-queries-500plus.csv
tail -n +2 batch7-methods.csv >> researchers-queries-500plus.csv
tail -n +2 batch8-career.csv >> researchers-queries-500plus.csv
tail -n +2 batch9-tools.csv >> researchers-queries-500plus.csv
tail -n +2 batch10-open-science.csv >> researchers-queries-500plus.csv

# Count total
wc -l researchers-queries-500plus.csv
```

### Step 4: Quality Control

**Quick Review Checklist:**
- [ ] All queries have numbered steps (1, 2, 3...)
- [ ] Queries are 150-300 words
- [ ] Target audiences are specific
- [ ] Priorities are distributed (not all HIGH)
- [ ] No duplicate queries
- [ ] CSV format is clean (no broken quotes)
- [ ] Categories match your structure

**Tools for QC:**
- Excel: Sort by category, scan for duplicates
- Find duplicates: `sort researchers-queries-500plus.csv | uniq -d`
- Count per category: `cut -d',' -f1 researchers-queries-500plus.csv | sort | uniq -c`

---

## ⚡ Time Estimate

### Realistic Timeline:

**Session 1 (2 hours):**
- Generate Batches 1-5 (250 queries)
- Quick review and save

**Session 2 (2 hours):**
- Generate Batches 6-10 (250 queries)
- Quick review and save

**Session 3 (1 hour):**
- Combine all CSVs
- Quality control
- Final review

**Total: 5 hours to 500+ queries** ✅

### Batch Processing Tips:

1. **Use Multiple AI Models**
   - Generate same batch from ChatGPT and Claude
   - Pick best queries from each
   - Merge for variety

2. **Parallel Generation**
   - Run 2-3 prompts simultaneously in different tabs
   - Reduces waiting time

3. **Save Incrementally**
   - Don't lose work
   - Save each batch immediately

---

## 📊 Expected Output

### Final CSV Structure:
```
researchers-queries-500plus.csv

Columns:
- category
- subcategory
- query (150-300 words with numbered steps)
- target_audience
- priority (HIGH/MEDIUM/LOW)
- use_academic_wrapper (TRUE/FALSE)
- notes

Total rows: 500+ (1 header + 500+ data rows)
File size: ~500KB
```

### Distribution Goal:
- Literature Review: 50 queries
- Quantitative Analysis: 50 queries
- Qualitative Analysis: 50 queries
- Mixed Methods: 40 queries
- Academic Writing: 60 queries
- Grant Writing: 40 queries
- Research Methods: 50 queries
- Career Development: 50 queries
- Tools & Software: 40 queries
- Open Science: 40 queries
- **Total: 470+ queries** (plus your existing 54 = 524 total)

---

## 🔄 After Generation: Automation

Once you have 500+ queries:

**Option 1: Manual**
- Pick top 50 HIGH priority queries
- Run through SNEOS.com manually
- Create comparisons with academic wrapper

**Option 2: Semi-Automated**
- Use SNEOS API (if available)
- Batch process queries
- Review outputs before publishing

**Option 3: Fully Automated**
- Script to run all queries through API
- Auto-generate markdown files
- Manual review and publish in batches

---

## 🎯 Success Metrics

### Phase 1: Query Generation (This Week)
- [ ] 500+ queries generated
- [ ] All 10 categories covered
- [ ] Quality reviewed
- [ ] CSV file ready

### Phase 2: Comparison Creation (Weeks 2-4)
- [ ] 50 Phase 1 comparisons created
- [ ] Landing page links updated
- [ ] Category pages populated

### Phase 3: Scale (Month 2)
- [ ] 200+ comparisons published
- [ ] Automated workflow established
- [ ] SEO improvements visible

---

## 📞 Questions & Troubleshooting

### Q: What if AI generates low-quality queries?
**A:**
- Refine the prompt with more specific examples
- Use "Generate 10 more like query #3" for good ones
- Manually edit 20-30% of queries

### Q: What if there are duplicates?
**A:**
```bash
# Find exact duplicates
sort researchers-queries-500plus.csv | uniq -d

# Find similar (manual review needed)
# Review queries with same subcategory
```

### Q: Should I review all 500 before using?
**A:**
- Quick scan: Yes (30-60 min)
- Deep edit: No (use as-is for most)
- Edit as you generate comparisons

### Q: What if I want 1000+ queries?
**A:**
- Run each batch 2x with different prompts
- Add more categories (Teaching, Administration, International Research)
- Generate discipline-specific variations

---

## 🎉 You're Ready!

**Next Action:**
1. Open ChatGPT or Claude
2. Copy the master prompt template
3. Run Batch 1: Literature Review (50 queries)
4. Save output as CSV
5. Repeat for remaining 9 batches
6. Combine into single file

**Time to Complete:** 4-6 hours across 2-3 sessions

**Result:** 500+ high-quality research queries ready for automation!

---

Good luck! This will transform your researchers section into the most comprehensive academic AI comparison library on the internet. 🚀
